{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fnil\fcharset0 Menlo-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red47\green180\blue29;\red180\green36\blue25;
\red64\green11\blue217;\red46\green174\blue187;\red255\green255\blue255;\red153\green153\blue153;\red230\green230\blue230;
}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c20238\c73898\c14947;\cssrgb\c76409\c21698\c12524;
\cssrgb\c32308\c18668\c88227;\cssrgb\c20196\c73240\c78250;\csgray\c100000;\csgenericrgb\c60000\c60000\c60000;\csgenericrgb\c90000\c90000\c90000;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww19080\viewh22000\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs36 \cf2 \CocoaLigature0 Last login: Tue Mar  9 19:42:59 on ttys002\
gez@miMac ~ % ls\
Applications				Pictures\
Desktop					Public\
Documents				Sites\
Downloads				VirtualBox VMs\
Library					documents \
Movies					https-github.com-Gerrymelb-mycyberfolio\
Music					monu-mel-cyber-pt-11-2020-u-c\
Parallels				prework GK\
gez@miMac ~ % cd monu-mel-cyber-pt-11-2020-u-c \
gez@miMac monu-mel-cyber-pt-11-2020-u-c % git pull\
remote: Enumerating objects: 180, done.\
remote: Counting objects: 100% (180/180), done.\
remote: Compressing objects: 100% (147/147), done.\
remote: Total 173 (delta 13), reused 136 (delta 3), pack-reused 0\
Receiving objects: 100% (173/173), 22.63 MiB | 3.48 MiB/s, done.\
Resolving deltas: 100% (13/13), completed with 5 local objects.\
From https://monash.bootcampcontent.com/monash-coding-bootcamp/monu-mel-cyber-pt-11-2020-u-c\
   97da8ef..fe314b6  master     -> origin/master\
Updating 97da8ef..fe314b6\
Fast-forward\
 .../03_Monolith_to_Microservices/Solved/README.md  |   68 \cf3 ++\cf2 \
 .../Solved/activity_one_solution.drawio            |    1 \cf3 +\cf2 \
 .../Solved/activity_one_solution_no_bonus.drawio   |    1 \cf3 +\cf2 \
 .../Unsolved/README.md                             |    0\
 .../Unsolved/activity_one_unsolved.drawio          |    0\
 .../Activities/07_Docker_Compose/Solved/README.md  |   52 \cf3 +\cf2 \
 .../07_Docker_Compose/Solved/docker-compose.yml    |   70 \cf3 ++\cf2 \
 .../07_Docker_Compose/Unsolved/README.md           |    0\
 .../07_Docker_Compose/Unsolved/docker-compose.yml  |    0\
 .../07_Docker_Compose/Unsolved/volume/importme.txt |    0\
 .../11_Database_Management/Solved/README.md        |   82 \cf3 ++\cf2 \
 .../11_Database_Management/Unsolved/README.md      |    0\
 .../14 - Web Development/2/StudentGuide.md         |    3 \cf4 -\cf2 \
 1 - Lesson Plans/14 - Web Development/README.md    |  106 \cf3 ++\cf2 \
 .../1/15.1 Slides.pdf                              |  Bin \cf4 0\cf2  -> \cf3 8562381\cf2  bytes\
 .../03_URL_Cruise_Missile/Unsolved/README.md       |   52 \cf3 +\cf2 \
 .../06_Client_Server_Attacks/Unsolved/README.md    |   85 \cf3 ++\cf2 \
 .../10_Information_Supermarket/Unsolved/README.md  |   50 \cf3 +\cf2 \
 .../Unsolved/Images/proxy_disable.png              |  Bin \cf4 0\cf2  -> \cf3 55901\cf2  bytes\
 .../13_Executing_Exploits/Unsolved/README.md       |  142 \cf3 +++\cf2 \
 .../1/Images/3-Tier-Server-Architecture.png        |  Bin \cf4 0\cf2  -> \cf3 30700\cf2  bytes\
 .../1/Images/Hashed Password Auth.png              |  Bin \cf4 0\cf2  -> \cf3 606625\cf2  bytes\
 .../1/Images/URL Cruise Missle.png                 |  Bin \cf4 0\cf2  -> \cf3 208883\cf2  bytes\
 .../1/Images/WAF.png                               |  Bin \cf4 0\cf2  -> \cf3 149139\cf2  bytes\
 .../1/Images/WebAppStructure.png                   |  Bin \cf4 0\cf2  -> \cf3 132847\cf2  bytes\
 .../1/Images/XSS_1.png                             |  Bin \cf4 0\cf2  -> \cf3 81905\cf2  bytes\
 .../1/Images/XSS_2.png                             |  Bin \cf4 0\cf2  -> \cf3 106473\cf2  bytes\
 .../1/Images/XSS_3.png                             |  Bin \cf4 0\cf2  -> \cf3 125781\cf2  bytes\
 .../1/Images/XSS_4.png                             |  Bin \cf4 0\cf2  -> \cf3 93927\cf2  bytes\
 .../1/Images/proxy_disable.png                     |  Bin \cf4 0\cf2  -> \cf3 55901\cf2  bytes\
 .../1/Images/users-by-social-media-platform.png    |  Bin \cf4 0\cf2  -> \cf3 498478\cf2  bytes\
 .../1/StudentGuide.md                              | 1076 \cf3 ++++++++++++++++++++\cf2 \
 .../2/15.2 Slides.pdf                              |  Bin \cf4 0\cf2  -> \cf3 2945776\cf2  bytes\
 .../03_Mapping_the_Database/Unsolved/README.md     |   49 \cf3 +\cf2 \
 .../07_BeEF/Unsolved/Images/landing_page.png       |  Bin \cf4 0\cf2  -> \cf3 169055\cf2  bytes\
 .../2/Activities/07_BeEF/Unsolved/README.md        |   52 \cf3 +\cf2 \
 .../09_Command_Injection/Unsolved/README.md        |   57 \cf3 ++\cf2 \
 .../2/Images/BeEF.png                              |  Bin \cf4 0\cf2  -> \cf3 222289\cf2  bytes\
 .../2/Images/BeEFDiagram.png                       |  Bin \cf4 0\cf2  -> \cf3 160631\cf2  bytes\
 .../2/Images/BeEF_Geo_Location_Results.png         |  Bin \cf4 0\cf2  -> \cf3 69344\cf2  bytes\
 .../2/Images/BeEF_Get_Geo_Location.png             |  Bin \cf4 0\cf2  -> \cf3 189867\cf2  bytes\
 .../2/Images/BeEF_Hooked_Browsers.png              |  Bin \cf4 0\cf2  -> \cf3 126797\cf2  bytes\
 .../2/Images/BeEF_Ifconfig.png                     |  Bin \cf4 0\cf2  -> \cf3 187978\cf2  bytes\
 .../2/Images/BeEF_JavaScript.png                   |  Bin \cf4 0\cf2  -> \cf3 106390\cf2  bytes\
 .../2/Images/BeEF_Start.png                        |  Bin \cf4 0\cf2  -> \cf3 397895\cf2  bytes\
 .../2/Images/CIA Triad.png                         |  Bin \cf4 0\cf2  -> \cf3 40193\cf2  bytes\
 .../2/Images/DNS_Lookup_Screen.png                 |  Bin \cf4 0\cf2  -> \cf3 191879\cf2  bytes\
 .../2/Images/Date_and_LS_commands.png              |  Bin \cf4 0\cf2  -> \cf3 94748\cf2  bytes\
 .../2/Images/SQL_Banner.png                        |  Bin \cf4 0\cf2  -> \cf3 661723\cf2  bytes\
 .../2/Images/SQLmap.png                            |  Bin \cf4 0\cf2  -> \cf3 115733\cf2  bytes\
 .../2/Images/Secondary_injection_commands.png      |  Bin \cf4 0\cf2  -> \cf3 119564\cf2  bytes\
 .../2/Images/WAF.png                               |  Bin \cf4 0\cf2  -> \cf3 74784\cf2  bytes\
 .../2/Images/nc_listener.png                       |  Bin \cf4 0\cf2  -> \cf3 39880\cf2  bytes\
 .../2/Images/reverse_shell.png                     |  Bin \cf4 0\cf2  -> \cf3 84077\cf2  bytes\
 .../2/Images/shell.png                             |  Bin \cf4 0\cf2  -> \cf3 154312\cf2  bytes\
 .../2/StudentGuide.md                              |  805 \cf3 +++++++++++++++\cf2 \
 .../3/15.3 Slides.pdf                              |  Bin \cf4 0\cf2  -> \cf3 3582350\cf2  bytes\
 .../Unsolved/README.md                             |   47 \cf3 +\cf2 \
 .../Unsolved/README.md                             |   74 \cf3 ++\cf2 \
 .../Unsolved/Images/credit_cards-cracked.png       |  Bin \cf4 0\cf2  -> \cf3 487249\cf2  bytes\
 .../Unsolved/Images/foxy_proxy_scab.png            |  Bin \cf4 0\cf2  -> \cf3 95373\cf2  bytes\
 .../Unsolved/Images/original_defaced.png           |  Bin \cf4 0\cf2  -> \cf3 335838\cf2  bytes\
 .../Unsolved/Images/view_network_tcp.png           |  Bin \cf4 0\cf2  -> \cf3 275237\cf2  bytes\
 .../Unsolved/Images/webscarab_2nd_window.png       |  Bin \cf4 0\cf2  -> \cf3 181212\cf2  bytes\
 .../Unsolved/Images/webscarab_abolute_path.png     |  Bin \cf4 0\cf2  -> \cf3 269910\cf2  bytes\
 .../Unsolved/Images/webscarab_file_value_field.png |  Bin \cf4 0\cf2  -> \cf3 196215\cf2  bytes\
 .../Unsolved/Images/webscarab_find_command-1.png   |  Bin \cf4 0\cf2  -> \cf3 179965\cf2  bytes\
 .../Unsolved/Images/webscarab_find_command.png     |  Bin \cf4 0\cf2  -> \cf3 179965\cf2  bytes\
 .../Unsolved/Images/whoami_pwd.png                 |  Bin \cf4 0\cf2  -> \cf3 266301\cf2  bytes\
 .../Unsolved/Images/whoami_pwd_image.png           |  Bin \cf4 0\cf2  -> \cf3 168527\cf2  bytes\
 .../Activities/11_The_Challenge/Unsolved/README.md |  133 \cf3 +++\cf2 \
 .../3/Images/Basic_Authentication.png              |  Bin \cf4 0\cf2  -> \cf3 585287\cf2  bytes\
 .../3/Images/Bypass_Results.png                    |  Bin \cf4 0\cf2  -> \cf3 271036\cf2  bytes\
 .../3/Images/CIA Triad.png                         |  Bin \cf4 0\cf2  -> \cf3 40193\cf2  bytes\
 .../3/Images/Client_Side_Validation.png            |  Bin \cf4 0\cf2  -> \cf3 190049\cf2  bytes\
 .../3/Images/Code_Quality_FIXME.png                |  Bin \cf4 0\cf2  -> \cf3 177216\cf2  bytes\
 .../3/Images/Code_Quality_Logged_In.png            |  Bin \cf4 0\cf2  -> \cf3 328156\cf2  bytes\
 .../3/Images/Code_Quality_Overview.png             |  Bin \cf4 0\cf2  -> \cf3 337146\cf2  bytes\
 .../3/Images/Congrats.png                          |  Bin \cf4 0\cf2  -> \cf3 471172\cf2  bytes\
 .../3/Images/CyberChef_Basic_Authentication.png    |  Bin \cf4 0\cf2  -> \cf3 262898\cf2  bytes\
 .../3/Images/CyberChef_Overview.png                |  Bin \cf4 0\cf2  -> \cf3 381681\cf2  bytes\
 .../3/Images/CyberChef_Transaction.png             |  Bin \cf4 0\cf2  -> \cf3 368778\cf2  bytes\
 .../3/Images/DNS_Lookup_Screen.png                 |  Bin \cf4 0\cf2  -> \cf3 191879\cf2  bytes\
 .../3/Images/Denial of Service.png                 |  Bin \cf4 0\cf2  -> \cf3 116324\cf2  bytes\
 .../3/Images/Firefox_Config.png                    |  Bin \cf4 0\cf2  -> \cf3 70403\cf2  bytes\
 .../3/Images/FoxyProxy_Icon.png                    |  Bin \cf4 0\cf2  -> \cf3 267362\cf2  bytes\
 .../3/Images/Larry.png                             |  Bin \cf4 0\cf2  -> \cf3 68969\cf2  bytes\
 .../3/Images/Neville.png                           |  Bin \cf4 0\cf2  -> \cf3 136956\cf2  bytes\
 .../3/Images/Parameterized_Query.png               |  Bin \cf4 0\cf2  -> \cf3 540954\cf2  bytes\
 .../3/Images/SQL_Login.png                         |  Bin \cf4 0\cf2  -> \cf3 202232\cf2  bytes\
 .../3/Images/Shortcuts.png                         |  Bin \cf4 0\cf2  -> \cf3 213258\cf2  bytes\
 .../3/Images/Start_Tamper_data.png                 |  Bin \cf4 0\cf2  -> \cf3 230917\cf2  bytes\
 .../3/Images/Tamper_Data_cookie.png                |  Bin \cf4 0\cf2  -> \cf3 471823\cf2  bytes\
 .../3/Images/Tamper_Data_fristbox.png              |  Bin \cf4 0\cf2  -> \cf3 421832\cf2  bytes\
 .../3/Images/Username_Password.png                 |  Bin \cf4 0\cf2  -> \cf3 460572\cf2  bytes\
 .../3/Images/WebScarab_Overview.png                |  Bin \cf4 0\cf2  -> \cf3 226585\cf2  bytes\
 .../3/Images/admin.png                             |  Bin \cf4 0\cf2  -> \cf3 132764\cf2  bytes\
 .../3/Images/tamperdata.png                        |  Bin \cf4 0\cf2  -> \cf3 237518\cf2  bytes\
 .../3/Images/toggle.png                            |  Bin \cf4 0\cf2  -> \cf3 110673\cf2  bytes\
 .../3/StudentGuide.md                              |  686 \cf3 +++++++++++++\cf2 \
 .../README.md                                      |   62 \cf3 ++\cf2 \
 .../Unsolved/HTTP_Reference.md                     |  167 \cf3 +++\cf2 \
 .../Unsolved/Images/HTTP_web_client_server.png     |  Bin \cf4 0\cf2  -> \cf3 391687\cf2  bytes\
 .../14 - Web Development/Unsolved/README.md        |  340 \cf3 +++++++\cf2 \
 .../Unsolved/cURL_Reference.md                     |  109 \cf3 ++\cf2 \
 README.md                                          |   10 \cf3 +\cf4 -\cf2 \
 106 files changed, 4371 insertions(+), 8 deletions(-)\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/03_Monolith_to_Microservices/Solved/README.md\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/03_Monolith_to_Microservices/Solved/activity_one_solution.drawio\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/03_Monolith_to_Microservices/Solved/activity_one_solution_no_bonus.drawio\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/03_Monolith_to_Microservices/Unsolved/README.md (100%)\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/03_Monolith_to_Microservices/Unsolved/activity_one_unsolved.drawio (100%)\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/07_Docker_Compose/Solved/README.md\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/07_Docker_Compose/Solved/docker-compose.yml\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/07_Docker_Compose/Unsolved/README.md (100%)\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/07_Docker_Compose/Unsolved/docker-compose.yml (100%)\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/07_Docker_Compose/Unsolved/volume/importme.txt (100%)\
 create mode 100644 1 - Lesson Plans/14 - Web Development/2/Activities/11_Database_Management/Solved/README.md\
 rename 1 - Lesson Plans/14 - Web Development/2/\{Activity => Activities\}/11_Database_Management/Unsolved/README.md (100%)\
 create mode 100644 1 - Lesson Plans/14 - Web Development/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/15.1 Slides.pdf\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Activities/03_URL_Cruise_Missile/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Activities/06_Client_Server_Attacks/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Activities/10_Information_Supermarket/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Activities/13_Executing_Exploits/Unsolved/Images/proxy_disable.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Activities/13_Executing_Exploits/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/3-Tier-Server-Architecture.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/Hashed Password Auth.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/URL Cruise Missle.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/WAF.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/WebAppStructure.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/XSS_1.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/XSS_2.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/XSS_3.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/XSS_4.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/proxy_disable.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/Images/users-by-social-media-platform.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/1/StudentGuide.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/15.2 Slides.pdf\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Activities/03_Mapping_the_Database/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Activities/07_BeEF/Unsolved/Images/landing_page.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Activities/07_BeEF/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Activities/09_Command_Injection/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEFDiagram.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_Geo_Location_Results.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_Get_Geo_Location.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_Hooked_Browsers.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_Ifconfig.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_JavaScript.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/BeEF_Start.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/CIA Triad.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/DNS_Lookup_Screen.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/Date_and_LS_commands.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/SQL_Banner.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/SQLmap.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/Secondary_injection_commands.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/WAF.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/nc_listener.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/reverse_shell.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/Images/shell.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/2/StudentGuide.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/15.3 Slides.pdf\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/03_Javascript_Validation_Bypass/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/06_Broken_Authentication_Session/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/credit_cards-cracked.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/foxy_proxy_scab.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/original_defaced.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/view_network_tcp.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/webscarab_2nd_window.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/webscarab_abolute_path.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/webscarab_file_value_field.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/webscarab_find_command-1.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/webscarab_find_command.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/whoami_pwd.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/Images/whoami_pwd_image.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Activities/11_The_Challenge/Unsolved/README.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Basic_Authentication.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Bypass_Results.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/CIA Triad.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Client_Side_Validation.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Code_Quality_FIXME.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Code_Quality_Logged_In.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Code_Quality_Overview.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Congrats.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/CyberChef_Basic_Authentication.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/CyberChef_Overview.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/CyberChef_Transaction.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/DNS_Lookup_Screen.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Denial of Service.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Firefox_Config.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/FoxyProxy_Icon.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Larry.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Neville.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Parameterized_Query.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/SQL_Login.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Shortcuts.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Start_Tamper_data.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Tamper_Data_cookie.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Tamper_Data_fristbox.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/Username_Password.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/WebScarab_Overview.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/admin.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/tamperdata.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/Images/toggle.png\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/3/StudentGuide.md\
 create mode 100644 1 - Lesson Plans/15 - Web Vulnerabilities and Hardening/README.md\
 create mode 100644 2 - Homework/14 - Web Development/Unsolved/HTTP_Reference.md\
 create mode 100644 2 - Homework/14 - Web Development/Unsolved/Images/HTTP_web_client_server.png\
 create mode 100644 2 - Homework/14 - Web Development/Unsolved/README.md\
 create mode 100644 2 - Homework/14 - Web Development/Unsolved/cURL_Reference.md\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % ssh azadmin@104.42.214.9        \
ssh: connect to host 104.42.214.9 port 22: Connection refused\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % ssh azadmin@23100.39.172             \
ssh: Could not resolve hostname 23100.39.172: nodename nor servname provided, or not known\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % ssh azadmin@23.100.39.172\
The authenticity of host '23.100.39.172 (23.100.39.172)' can't be established.\
ECDSA key fingerprint is SHA256:rpu4NdxYIWK+ywvgdAIVSXwyK8XwiNuRbL3YcP8nP3g.\
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes\
Warning: Permanently added '23.100.39.172' (ECDSA) to the list of known hosts.\
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-azure x86_64)\
\
 * Documentation:  https://help.ubuntu.com\
 * Management:     https://landscape.canonical.com\
 * Support:        https://ubuntu.com/advantage\
\
  System information as of Wed Mar 10 09:14:04 UTC 2021\
\
  System load:  0.0                Processes:           117\
  Usage of /:   19.7% of 28.90GB   Users logged in:     0\
  Memory usage: 3%                 IP address for eth0: 10.0.0.5\
  Swap usage:   0%\
\
 * Introducing self-healing high availability clusters in MicroK8s.\
   Simple, hardened, Kubernetes for production, from RaspberryPi to DC.\
\
     https://microk8s.io/high-availability\
\
 * Canonical Livepatch is available for installation.\
   - Reduce system reboots and improve kernel security. Activate at:\
     https://ubuntu.com/livepatch\
\
15 packages can be updated.\
0 of these updates are security updates.\
To see these additional updates run: apt list --upgradable\
\
\
Last login: Tue Mar  9 08:44:46 2021 from 172.197.53.164\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker start xenodochial_chaplygin\
xenodochial_chaplygin\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker attach xenodochial_chaplygin\
root@d519ff78ed01:~# docker ps\
bash: docker: command not found\
root@d519ff78ed01:~# docker -ps\
bash: docker: command not found\
root@d519ff78ed01:~# docker ps -a\
bash: docker: command not found\
root@d519ff78ed01:~# docker ps -a\
bash: docker: command not found\
root@d519ff78ed01:~# exit\
exit\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ docker ps -a\
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker ps -a\
CONTAINER ID        IMAGE                           COMMAND             CREATED             STATUS                        PORTS               NAMES\
d519ff78ed01        cyberxsecurity/ansible:latest   "bash"              11 days ago         Exited (127) 17 seconds ago                       xenodochial_chaplygin\
315e978ba72c        cyberxsecurity/ansible          "/bin/bash"         13 days ago         Exited (0) 11 days ago                            zealous_mcclintock\
8f170c0ae3ad        cyberxsecurity/ansible          "/bin/bash"         13 days ago         Exited (0) 13 days ago                            gracious_blackwell\
3a4a3a3136e2        cyberxsecurity/ansible:latest   "bash"              13 days ago         Exited (0) 13 days ago                            goofy_lehmann\
d91840a6487a        cyberxsecurity/ansible:latest   "bash"              2 weeks ago         Exited (255) 2 weeks ago                          bold_lalande\
b502ca8ab9ee        cyberxsecurity/ansible:latest   "bash"              2 weeks ago         Exited (0) 2 weeks ago                            upbeat_leavitt\
031d45168656        cyberxsecurity/ubuntu:bionic    "bash"              2 weeks ago         Exited (1) 2 weeks ago                            funny_williams\
01e6e4c114da        cyberxsecurity/ubuntu:bionic    "bas"               2 weeks ago         Created                                           elegant_mclean\
9d791914d3bb        cyberxsecurity/ansible:latest   "bash"              2 weeks ago         Exited (0) 2 weeks ago                            peaceful_greider\
d66a4526391a        cyberxsecurity/ansible:latest   "bash"              2 weeks ago         Exited (0) 2 weeks ago                            musing_edison\
161db8d7789d        cyberxsecurity/ansible:latest   "bash"              2 weeks ago         Exited (127) 2 weeks ago                          upbeat_chebyshev\
b61fe845db4d        cyberxsecurity/ubuntu:bionic    "bash"              2 weeks ago         Exited (0) 2 weeks ago                            intelligent_kalam\
1cda8f9bf830        cyberxsecurity/ubuntu:bionic    "bash"              2 weeks ago         Exited (127) 2 weeks ago                          sharp_keller\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ssh azadmin@40.76.129.141\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ssh azadmin@10.3.0.4\
The authenticity of host '10.3.0.4 (10.3.0.4)' can't be established.\
ECDSA key fingerprint is SHA256:QqqvCGveElx7DG27O/GaQi81d1TydEuPCawFGqLh7RU.\
Are you sure you want to continue connecting (yes/no)? yes\
Warning: Permanently added '10.3.0.4' (ECDSA) to the list of known hosts.\
azadmin@10.3.0.4: Permission denied (publickey).\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker attach xenodochial_chaplygin\
You cannot attach to a stopped container, start it first\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker start xenodochial_chaplygin\
xenodochial_chaplygin\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker attach xenodochial_chaplygin\
root@d519ff78ed01:~# ssh azadmin@10.3.0.4\
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-azure x86_64)\
\
 * Documentation:  https://help.ubuntu.com\
 * Management:     https://landscape.canonical.com\
 * Support:        https://ubuntu.com/advantage\
\
  System information as of Wed Mar 10 09:18:38 UTC 2021\
\
  System load:  0.0                Processes:              129\
  Usage of /:   17.8% of 28.90GB   Users logged in:        0\
  Memory usage: 35%                IP address for eth0:    10.3.0.4\
  Swap usage:   0%                 IP address for docker0: 172.17.0.1\
\
 * Introducing self-healing high availability clusters in MicroK8s.\
   Simple, hardened, Kubernetes for production, from RaspberryPi to DC.\
\
     https://microk8s.io/high-availability\
\
 * Canonical Livepatch is available for installation.\
   - Reduce system reboots and improve kernel security. Activate at:\
     https://ubuntu.com/livepatch\
\
15 packages can be updated.\
0 of these updates are security updates.\
To see these additional updates run: apt list --upgradable\
\
\
Last login: Mon Mar  8 10:32:21 2021 from 10.0.0.5\
azadmin@ProjVm:~$ docker ps -a\
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied\
azadmin@ProjVm:~$ sudo docker ps -a\
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                  PORTS                                                                              NAMES\
522521fefe2d        hello-world         "/hello"                 6 days ago          Exited (0) 6 days ago                                                                                      condescending_cartwright\
00e79970a6cf        sebp/elk:761        "/usr/local/bin/star\'85"   6 days ago          Up 3 hours              0.0.0.0:5044->5044/tcp, 0.0.0.0:5601->5601/tcp, 0.0.0.0:9200->9200/tcp, 9300/tcp   elk\
azadmin@ProjVm:~$ exit\
logout\
Connection to 10.3.0.4 closed.\
root@d519ff78ed01:~# cd etc \
bash: cd: etc: No such file or directory\
root@d519ff78ed01:~# cd /etc\
root@d519ff78ed01:/etc# cd ansible\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# cat filebeat-config.yml \
######################## Filebeat Configuration ############################\
# This file is a full configuration example documenting all non-deprecated\
# options in comments. For a shorter configuration example, that contains only\
# the most common options, please see filebeat.yml in the same directory.\
#\
# You can find the full configuration reference here:\
# https://www.elastic.co/guide/en/beats/filebeat/index.html\
filebeat.config.modules:\
  path: $\{path.config\}/modules.d/*.yml\
\
#==========================  Modules configuration =============================\
filebeat.modules:\
\
#-------------------------------- System Module --------------------------------\
#- module: system\
  # Syslog\
  #syslog:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Authorization logs\
  #auth:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#-------------------------------- Apache Module --------------------------------\
#- module: apache\
  # Access logs\
  #access:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Error logs\
  #error:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
#-------------------------------- Auditd Module --------------------------------\
#- module: auditd\
  #log:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#---------------------------- Elasticsearch Module ----------------------------\
- module: elasticsearch\
  # Server log\
  server:\
    enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
  gc:\
    enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
  audit:\
    enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
  slowlog:\
    enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
  deprecation:\
    enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#------------------------------- Haproxy Module -------------------------------\
- module: haproxy\
  # All logs\
  log:\
    enabled: true\
\
    # Set which input to use between syslog (default) or file.\
    #var.input:\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#-------------------------------- Icinga Module --------------------------------\
#- module: icinga\
  # Main logs\
  #main:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Debug logs\
  #debug:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Startup logs\
  #startup:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#--------------------------------- IIS Module ---------------------------------\
#- module: iis\
  # Access logs\
  #access:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Error logs\
  #error:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#-------------------------------- Kafka Module --------------------------------\
- module: kafka\
  # All logs\
  log:\
    enabled: true\
\
    # Set custom paths for Kafka. If left empty,\
    # Filebeat will look under /opt.\
    #var.kafka_home:\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#-------------------------------- Kibana Module --------------------------------\
- module: kibana\
  # All logs\
  log:\
    enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#------------------------------- Logstash Module -------------------------------\
#- module: logstash\
  # logs\
  #log:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    # var.paths:\
\
  # Slow logs\
  #slowlog:\
    #enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#------------------------------- Mongodb Module -------------------------------\
#- module: mongodb\
  # Logs\
  #log:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#-------------------------------- MySQL Module --------------------------------\
#- module: mysql\
  # Error logs\
  #error:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Slow logs\
  #slowlog:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#--------------------------------- Nats Module ---------------------------------\
- module: nats\
  # All logs\
  log:\
    enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
#-------------------------------- Nginx Module --------------------------------\
#- module: nginx\
  # Access logs\
  #access:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
  # Error logs\
  #error:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#------------------------------- Osquery Module -------------------------------\
- module: osquery\
  result:\
    enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # If true, all fields created by this module are prefixed with\
    # `osquery.result`. Set to false to copy the fields in the root\
    # of the document. The default is true.\
    #var.use_namespace: true\
\
#------------------------------ PostgreSQL Module ------------------------------\
#- module: postgresql\
  # Logs\
  #log:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
#-------------------------------- Redis Module --------------------------------\
#- module: redis\
  # Main logs\
  #log:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths: ["/var/log/redis/redis-server.log*"]\
\
  # Slow logs, retrieved via the Redis API (SLOWLOG)\
  #slowlog:\
    #enabled: true\
\
    # The Redis hosts to connect to.\
    #var.hosts: ["localhost:6379"]\
\
    # Optional, the password to use when connecting to Redis.\
    #var.password:\
\
#----------------------------- Google Santa Module -----------------------------\
- module: santa\
  log:\
    enabled: true\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the the default path.\
    #var.paths:\
\
#------------------------------- Traefik Module -------------------------------\
#- module: traefik\
  # Access logs\
  #access:\
    #enabled: true\
\
    # Set custom paths for the log files. If left empty,\
    # Filebeat will choose the paths depending on your OS.\
    #var.paths:\
\
    # Input configuration (advanced). Any input configuration option\
    # can be added under this section.\
    #input:\
\
\
#=========================== Filebeat inputs =============================\
\
# List of inputs to fetch data.\
filebeat.inputs:\
# Each - is an input. Most options can be set at the input level, so\
# you can use different inputs for various configurations.\
# Below are the input specific configurations.\
\
# Type of the files. Based on this the way the file is read is decided.\
# The different types cannot be mixed in one input\
#\
# Possible options are:\
# * log: Reads every line of the log file (default)\
# * stdin: Reads the standard in\
\
#------------------------------ Log input --------------------------------\
- type: log\
\
  # Change to true to enable this input configuration.\
  enabled: false\
\
  # Paths that should be crawled and fetched. Glob based paths.\
  # To fetch all ".log" files from a specific level of subdirectories\
  # /var/log/*/*.log can be used.\
  # For each file found under this path, a harvester is started.\
  # Make sure not file is defined twice as this can lead to unexpected behaviour.\
  paths:\
    - /var/log/*.log\
    #- c:\\programdata\\elasticsearch\\logs\\*\
\
  # Configure the file encoding for reading files with international characters\
  # following the W3C recommendation for HTML5 (http://www.w3.org/TR/encoding).\
  # Some sample encodings:\
  #   plain, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk,\
  #    hz-gb-2312, euc-kr, euc-jp, iso-2022-jp, shift-jis, ...\
  #encoding: plain\
\
\
  # Exclude lines. A list of regular expressions to match. It drops the lines that are\
  # matching any regular expression from the list. The include_lines is called before\
  # exclude_lines. By default, no lines are dropped.\
  #exclude_lines: ['^DBG']\
\
  # Include lines. A list of regular expressions to match. It exports the lines that are\
  # matching any regular expression from the list. The include_lines is called before\
  # exclude_lines. By default, all the lines are exported.\
  #include_lines: ['^ERR', '^WARN']\
\
  # Exclude files. A list of regular expressions to match. Filebeat drops the files that\
  # are matching any regular expression from the list. By default, no files are dropped.\
  #exclude_files: ['.gz$']\
\
  # Optional additional fields. These fields can be freely picked\
  # to add additional information to the crawled log files for filtering\
  #fields:\
  #  level: debug\
  #  review: 1\
\
  # Set to true to store the additional fields as top level fields instead\
  # of under the "fields" sub-dictionary. In case of name conflicts with the\
  # fields added by Filebeat itself, the custom fields overwrite the default\
  # fields.\
  #fields_under_root: false\
\
  # Set to true to publish fields with null values in events.\
  #keep_null: false\
\
  # Ignore files which were modified more then the defined timespan in the past.\
  # ignore_older is disabled by default, so no files are ignored by setting it to 0.\
  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.\
  #ignore_older: 0\
\
  # How often the input checks for new files in the paths that are specified\
  # for harvesting. Specify 1s to scan the directory as frequently as possible\
  # without causing Filebeat to scan too frequently. Default: 10s.\
  #scan_frequency: 10s\
\
  # Defines the buffer size every harvester uses when fetching the file\
  #harvester_buffer_size: 16384\
\
  # Maximum number of bytes a single log event can have\
  # All bytes after max_bytes are discarded and not sent. The default is 10MB.\
  # This is especially useful for multiline log messages which can get large.\
  #max_bytes: 10485760\
\
  # Characters which separate the lines. Valid values: auto, line_feed, vertical_tab, form_feed,\
  # carriage_return, carriage_return_line_feed, next_line, line_separator, paragraph_separator.\
  #line_terminator: auto\
\
  ### Recursive glob configuration\
\
  # Expand "**" patterns into regular glob patterns.\
  #recursive_glob.enabled: true\
\
  ### JSON configuration\
\
  # Decode JSON options. Enable this if your logs are structured in JSON.\
  # JSON key on which to apply the line filtering and multiline settings. This key\
  # must be top level and its value must be string, otherwise it is ignored. If\
  # no text key is defined, the line filtering and multiline features cannot be used.\
  #json.message_key:\
\
  # By default, the decoded JSON is placed under a "json" key in the output document.\
  # If you enable this setting, the keys are copied top level in the output document.\
  #json.keys_under_root: false\
\
  # If keys_under_root and this setting are enabled, then the values from the decoded\
  # JSON object overwrite the fields that Filebeat normally adds (type, source, offset, etc.)\
  # in case of conflicts.\
  #json.overwrite_keys: false\
\
  # If this setting is enabled, Filebeat adds a "error.message" and "error.key: json" key in case of JSON\
  # unmarshaling errors or when a text key is defined in the configuration but cannot\
  # be used.\
  #json.add_error_key: false\
\
  ### Multiline options\
\
  # Multiline can be used for log messages spanning multiple lines. This is common\
  # for Java Stack Traces or C-Line Continuation\
\
  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [\
  #multiline.pattern: ^\\[\
\
  # Defines if the pattern set under pattern should be negated or not. Default is false.\
  #multiline.negate: false\
\
  # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern\
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.\
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash\
  #multiline.match: after\
\
  # The maximum number of lines that are combined to one event.\
  # In case there are more the max_lines the additional lines are discarded.\
  # Default is 500\
  #multiline.max_lines: 500\
\
  # After the defined timeout, an multiline event is sent even if no new pattern was found to start a new event\
  # Default is 5s.\
  #multiline.timeout: 5s\
\
  # Setting tail_files to true means filebeat starts reading new files at the end\
  # instead of the beginning. If this is used in combination with log rotation\
  # this can mean that the first entries of a new file are skipped.\
  #tail_files: false\
\
  # The Ingest Node pipeline ID associated with this input. If this is set, it\
  # overwrites the pipeline option from the Elasticsearch output.\
  #pipeline:\
\
  # If symlinks is enabled, symlinks are opened and harvested. The harvester is opening the\
  # original for harvesting but will report the symlink name as source.\
  #symlinks: false\
\
  # Backoff values define how aggressively filebeat crawls new files for updates\
  # The default values can be used in most cases. Backoff defines how long it is waited\
  # to check a file again after EOF is reached. Default is 1s which means the file\
  # is checked every second if new lines were added. This leads to a near real time crawling.\
  # Every time a new line appears, backoff is reset to the initial value.\
  #backoff: 1s\
\
  # Max backoff defines what the maximum backoff time is. After having backed off multiple times\
  # from checking the files, the waiting time will never exceed max_backoff independent of the\
  # backoff factor. Having it set to 10s means in the worst case a new line can be added to a log\
  # file after having backed off multiple times, it takes a maximum of 10s to read the new line\
  #max_backoff: 10s\
\
  # The backoff factor defines how fast the algorithm backs off. The bigger the backoff factor,\
  # the faster the max_backoff value is reached. If this value is set to 1, no backoff will happen.\
  # The backoff value will be multiplied each time with the backoff_factor until max_backoff is reached\
  #backoff_factor: 2\
\
  # Max number of harvesters that are started in parallel.\
  # Default is 0 which means unlimited\
  #harvester_limit: 0\
\
  ### Harvester closing options\
\
  # Close inactive closes the file handler after the predefined period.\
  # The period starts when the last line of the file was, not the file ModTime.\
  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.\
  #close_inactive: 5m\
\
  # Close renamed closes a file handler when the file is renamed or rotated.\
  # Note: Potential data loss. Make sure to read and understand the docs for this option.\
  #close_renamed: false\
\
  # When enabling this option, a file handler is closed immediately in case a file can't be found\
  # any more. In case the file shows up again later, harvesting will continue at the last known position\
  # after scan_frequency.\
  #close_removed: true\
\
  # Closes the file handler as soon as the harvesters reaches the end of the file.\
  # By default this option is disabled.\
  # Note: Potential data loss. Make sure to read and understand the docs for this option.\
  #close_eof: false\
\
  ### State options\
\
  # Files for the modification data is older then clean_inactive the state from the registry is removed\
  # By default this is disabled.\
  #clean_inactive: 0\
\
  # Removes the state for file which cannot be found on disk anymore immediately\
  #clean_removed: true\
\
  # Close timeout closes the harvester after the predefined time.\
  # This is independent if the harvester did finish reading the file or not.\
  # By default this option is disabled.\
  # Note: Potential data loss. Make sure to read and understand the docs for this option.\
  #close_timeout: 0\
\
  # Defines if inputs is enabled\
  #enabled: true\
\
#----------------------------- Stdin input -------------------------------\
# Configuration to use stdin input\
#- type: stdin\
\
#------------------------- Redis slowlog input ---------------------------\
# Experimental: Config options for the redis slow log input\
#- type: redis\
  #enabled: false\
\
  # List of hosts to pool to retrieve the slow log information.\
  #hosts: ["localhost:6379"]\
\
  # How often the input checks for redis slow log.\
  #scan_frequency: 10s\
\
  # Timeout after which time the input should return an error\
  #timeout: 1s\
\
  # Network type to be used for redis connection. Default: tcp\
  #network: tcp\
\
  # Max number of concurrent connections. Default: 10\
  #maxconn: 10\
\
  # Redis AUTH password. Empty by default.\
  #password: foobared\
\
#------------------------------ Udp input --------------------------------\
# Experimental: Config options for the udp input\
#- type: udp\
  #enabled: false\
\
  # Maximum size of the message received over UDP\
  #max_message_size: 10KiB\
\
  # Size of the UDP read buffer in bytes\
  #read_buffer: 0\
\
\
#------------------------------ TCP input --------------------------------\
# Experimental: Config options for the TCP input\
#- type: tcp\
  #enabled: false\
\
  # The host and port to receive the new event\
  #host: "localhost:9000"\
\
  # Character used to split new message\
  #line_delimiter: "\\n"\
\
  # Maximum size in bytes of the message received over TCP\
  #max_message_size: 20MiB\
\
  # Max number of concurrent connections, or 0 for no limit. Default: 0\
  #max_connections: 0\
\
  # The number of seconds of inactivity before a remote connection is closed.\
  #timeout: 300s\
\
  # Use SSL settings for TCP.\
  #ssl.enabled: true\
\
  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # SSL configuration. By default is off.\
  # List of root certificates for client verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL server authentication.\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Server Certificate Key,\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the Certificate Key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections.\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE based cipher suites.\
  #ssl.curve_types: []\
\
  # Configure what types of client authentication are supported. Valid options\
  # are `none`, `optional`, and `required`. When `certificate_authorities` is set it will\
  # default to `required` otherwise it will be set to `none`.\
  #ssl.client_authentication: "required"\
\
#------------------------------ Syslog input --------------------------------\
# Experimental: Config options for the Syslog input\
# Accept RFC3164 formatted syslog event via UDP.\
#- type: syslog\
  #enabled: false\
  #protocol.udp:\
    # The host and port to receive the new event\
    #host: "localhost:9000"\
\
    # Maximum size of the message received over UDP\
    #max_message_size: 10KiB\
\
# Accept RFC3164 formatted syslog event via TCP.\
#- type: syslog\
  #enabled: false\
\
  #protocol.tcp:\
    # The host and port to receive the new event\
    #host: "localhost:9000"\
\
    # Character used to split new message\
    #line_delimiter: "\\n"\
\
    # Maximum size in bytes of the message received over TCP\
    #max_message_size: 20MiB\
\
    # The number of seconds of inactivity before a remote connection is closed.\
    #timeout: 300s\
\
    # Use SSL settings for TCP.\
    #ssl.enabled: true\
\
    # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\
    # 1.2 are enabled.\
    #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
    # SSL configuration. By default is off.\
    # List of root certificates for client verifications\
    #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
    # Certificate for SSL server authentication.\
    #ssl.certificate: "/etc/pki/client/cert.pem"\
\
    # Server Certificate Key,\
    #ssl.key: "/etc/pki/client/cert.key"\
\
    # Optional passphrase for decrypting the Certificate Key.\
    #ssl.key_passphrase: ''\
\
    # Configure cipher suites to be used for SSL connections.\
    #ssl.cipher_suites: []\
\
    # Configure curve types for ECDHE based cipher suites.\
    #ssl.curve_types: []\
\
    # Configure what types of client authentication are supported. Valid options\
    # are `none`, `optional`, and `required`. When `certificate_authorities` is set it will\
    # default to `required` otherwise it will be set to `none`.\
    #ssl.client_authentication: "required"\
\
#------------------------------ Container input --------------------------------\
#- type: container\
  #enabled: false\
\
  # Paths for container logs that should be crawled and fetched.\
  #paths:\
  #  -/var/lib/docker/containers/*/*.log\
\
  # Configure stream to filter to a specific stream: stdout, stderr or all (default)\
  #stream: all\
\
#========================== Filebeat autodiscover ==============================\
\
# Autodiscover allows you to detect changes in the system and spawn new modules\
# or inputs as they happen.\
\
#filebeat.autodiscover:\
  # List of enabled autodiscover providers\
#  providers:\
#    - type: docker\
#      templates:\
#        - condition:\
#            equals.docker.container.image: busybox\
#          config:\
#            - type: container\
#              paths:\
#                - /var/lib/docker/containers/$\{data.docker.container.id\}/*.log\
\
#========================= Filebeat global options ============================\
\
# Registry data path. If a relative path is used, it is considered relative to the\
# data path.\
#filebeat.registry.path: $\{path.data\}/registry\
\
# The permissions mask to apply on registry data, and meta files. The default\
# value is 0600.  Must be a valid Unix-style file permissions mask expressed in\
# octal notation.  This option is not supported on Windows.\
#filebeat.registry.file_permissions: 0600\
\
# The timeout value that controls when registry entries are written to disk\
# (flushed). When an unwritten update exceeds this value, it triggers a write\
# to disk. When flush is set to 0s, the registry is written to disk after each\
# batch of events has been published successfully. The default value is 0s.\
#filebeat.registry.flush: 0s\
\
\
# Starting with Filebeat 7.0, the registry uses a new directory format to store\
# Filebeat state. After you upgrade, Filebeat will automatically migrate a 6.x\
# registry file to use the new directory format. If you changed\
# filebeat.registry.path while upgrading, set filebeat.registry.migrate_file to\
# point to the old registry file.\
#filebeat.registry.migrate_file: $\{path.data\}/registry\
\
# By default Ingest pipelines are not updated if a pipeline with the same ID\
# already exists. If this option is enabled Filebeat overwrites pipelines\
# everytime a new Elasticsearch connection is established.\
#filebeat.overwrite_pipelines: false\
\
# How long filebeat waits on shutdown for the publisher to finish.\
# Default is 0, not waiting.\
#filebeat.shutdown_timeout: 0\
\
# Enable filebeat config reloading\
#filebeat.config:\
  #inputs:\
    #enabled: false\
    #path: inputs.d/*.yml\
    #reload.enabled: true\
    #reload.period: 10s\
  #modules:\
    #enabled: false\
    #path: modules.d/*.yml\
    #reload.enabled: true\
    #reload.period: 10s\
\
#================================ General ======================================\
\
# The name of the shipper that publishes the network data. It can be used to group\
# all the transactions sent by a single shipper in the web interface.\
# If this options is not defined, the hostname is used.\
#name:\
\
# The tags of the shipper are included in their own field with each\
# transaction published. Tags make it easy to group servers by different\
# logical properties.\
#tags: ["service-X", "web-tier"]\
\
# Optional fields that you can specify to add additional information to the\
# output. Fields can be scalar values, arrays, dictionaries, or any nested\
# combination of these.\
#fields:\
#  env: staging\
\
# If this option is set to true, the custom fields are stored as top-level\
# fields in the output document instead of being grouped under a fields\
# sub-dictionary. Default is false.\
#fields_under_root: false\
\
# Internal queue configuration for buffering events to be published.\
#queue:\
  # Queue type by name (default 'mem')\
  # The memory queue will present all available events (up to the outputs\
  # bulk_max_size) to the output, the moment the output is ready to server\
  # another batch of events.\
  #mem:\
    # Max number of events the queue can buffer.\
    #events: 4096\
\
    # Hints the minimum number of events stored in the queue,\
    # before providing a batch of events to the outputs.\
    # The default value is set to 2048.\
    # A value of 0 ensures events are immediately available\
    # to be sent to the outputs.\
    #flush.min_events: 2048\
\
    # Maximum duration after which events are available to the outputs,\
    # if the number of events stored in the queue is < `flush.min_events`.\
    #flush.timeout: 1s\
\
  # The spool queue will store events in a local spool file, before\
  # forwarding the events to the outputs.\
  #\
  # Beta: spooling to disk is currently a beta feature. Use with care.\
  #\
  # The spool file is a circular buffer, which blocks once the file/buffer is full.\
  # Events are put into a write buffer and flushed once the write buffer\
  # is full or the flush_timeout is triggered.\
  # Once ACKed by the output, events are removed immediately from the queue,\
  # making space for new events to be persisted.\
  #spool:\
    # The file namespace configures the file path and the file creation settings.\
    # Once the file exists, the `size`, `page_size` and `prealloc` settings\
    # will have no more effect.\
    #file:\
      # Location of spool file. The default value is $\{path.data\}/spool.dat.\
      #path: "$\{path.data\}/spool.dat"\
\
      # Configure file permissions if file is created. The default value is 0600.\
      #permissions: 0600\
\
      # File size hint. The spool blocks, once this limit is reached. The default value is 100 MiB.\
      #size: 100MiB\
\
      # The files page size. A file is split into multiple pages of the same size. The default value is 4KiB.\
      #page_size: 4KiB\
\
      # If prealloc is set, the required space for the file is reserved using\
      # truncate. The default value is true.\
      #prealloc: true\
\
    # Spool writer settings\
    # Events are serialized into a write buffer. The write buffer is flushed if:\
    # - The buffer limit has been reached.\
    # - The configured limit of buffered events is reached.\
    # - The flush timeout is triggered.\
    #write:\
      # Sets the write buffer size.\
      #buffer_size: 1MiB\
\
      # Maximum duration after which events are flushed if the write buffer\
      # is not full yet. The default value is 1s.\
      #flush.timeout: 1s\
\
      # Number of maximum buffered events. The write buffer is flushed once the\
      # limit is reached.\
      #flush.events: 16384\
\
      # Configure the on-disk event encoding. The encoding can be changed\
      # between restarts.\
      # Valid encodings are: json, ubjson, and cbor.\
      #codec: cbor\
    #read:\
      # Reader flush timeout, waiting for more events to become available, so\
      # to fill a complete batch as required by the outputs.\
      # If flush_timeout is 0, all available events are forwarded to the\
      # outputs immediately.\
      # The default value is 0s.\
      #flush.timeout: 0s\
\
# Sets the maximum number of CPUs that can be executing simultaneously. The\
# default is the number of logical CPUs available in the system.\
#max_procs:\
\
#================================ Processors ===================================\
\
# Processors are used to reduce the number of fields in the exported event or to\
# enhance the event with external metadata. This section defines a list of\
# processors that are applied one by one and the first one receives the initial\
# event:\
#\
#   event -> filter1 -> event1 -> filter2 ->event2 ...\
#\
# The supported processors are drop_fields, drop_event, include_fields,\
# decode_json_fields, and add_cloud_metadata.\
#\
# For example, you can use the following processors to keep the fields that\
# contain CPU load percentages, but remove the fields that contain CPU ticks\
# values:\
#\
#processors:\
#- include_fields:\
#    fields: ["cpu"]\
#- drop_fields:\
#    fields: ["cpu.user", "cpu.system"]\
#\
# The following example drops the events that have the HTTP response code 200:\
#\
#processors:\
#- drop_event:\
#    when:\
#       equals:\
#           http.code: 200\
#\
# The following example renames the field a to b:\
#\
#processors:\
#- rename:\
#    fields:\
#       - from: "a"\
#         to: "b"\
#\
# The following example tokenizes the string into fields:\
#\
#processors:\
#- dissect:\
#    tokenizer: "%\{key1\} - %\{key2\}"\
#    field: "message"\
#    target_prefix: "dissect"\
#\
# The following example enriches each event with metadata from the cloud\
# provider about the host machine. It works on EC2, GCE, DigitalOcean,\
# Tencent Cloud, and Alibaba Cloud.\
#\
#processors:\
#- add_cloud_metadata: ~\
#\
# The following example enriches each event with the machine's local time zone\
# offset from UTC.\
#\
#processors:\
#- add_locale:\
#    format: offset\
#\
# The following example enriches each event with docker metadata, it matches\
# given fields to an existing container id and adds info from that container:\
#\
#processors:\
#- add_docker_metadata:\
#    host: "unix:///var/run/docker.sock"\
#    match_fields: ["system.process.cgroup.id"]\
#    match_pids: ["process.pid", "process.ppid"]\
#    match_source: true\
#    match_source_index: 4\
#    match_short_id: false\
#    cleanup_timeout: 60\
#    labels.dedot: false\
#    # To connect to Docker over TLS you must specify a client and CA certificate.\
#    #ssl:\
#    #  certificate_authority: "/etc/pki/root/ca.pem"\
#    #  certificate:           "/etc/pki/client/cert.pem"\
#    #  key:                   "/etc/pki/client/cert.key"\
#\
# The following example enriches each event with docker metadata, it matches\
# container id from log path available in `source` field (by default it expects\
# it to be /var/lib/docker/containers/*/*.log).\
#\
#processors:\
#- add_docker_metadata: ~\
#\
# The following example enriches each event with host metadata.\
#\
#processors:\
#- add_host_metadata:\
#   netinfo.enabled: false\
#\
# The following example enriches each event with process metadata using\
# process IDs included in the event.\
#\
#processors:\
#- add_process_metadata:\
#    match_pids: ["system.process.ppid"]\
#    target: system.process.parent\
#\
# The following example decodes fields containing JSON strings\
# and replaces the strings with valid JSON objects.\
#\
#processors:\
#- decode_json_fields:\
#    fields: ["field1", "field2", ...]\
#    process_array: false\
#    max_depth: 1\
#    target: ""\
#    overwrite_keys: false\
#\
#processors:\
#- decompress_gzip_field:\
#    from: "field1"\
#    to: "field2"\
#    ignore_missing: false\
#    fail_on_error: true\
#\
# The following example copies the value of message to message_copied\
#\
#processors:\
#- copy_fields:\
#    fields:\
#        - from: message\
#          to: message_copied\
#    fail_on_error: true\
#    ignore_missing: false\
#\
# The following example truncates the value of message to 1024 bytes\
#\
#processors:\
#- truncate_fields:\
#    fields:\
#      - message\
#    max_bytes: 1024\
#    fail_on_error: false\
#    ignore_missing: true\
#\
# The following example preserves the raw message under event.original\
#\
#processors:\
#- copy_fields:\
#    fields:\
#        - from: message\
#          to: event.original\
#    fail_on_error: false\
#    ignore_missing: true\
#- truncate_fields:\
#    fields:\
#      - event.original\
#    max_bytes: 1024\
#    fail_on_error: false\
#    ignore_missing: true\
\
#============================= Elastic Cloud ==================================\
\
# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).\
\
# The cloud.id setting overwrites the `output.elasticsearch.hosts` and\
# `setup.kibana.host` options.\
# You can find the `cloud.id` in the Elastic Cloud web UI.\
#cloud.id:\
\
# The cloud.auth setting overwrites the `output.elasticsearch.username` and\
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.\
#cloud.auth:\
\
#================================ Outputs ======================================\
\
# Configure what output to use when sending the data collected by the beat.\
\
#-------------------------- Elasticsearch output -------------------------------\
output.elasticsearch:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # Array of hosts to connect to.\
  # Scheme and port can be left out and will be set to the default (http and 9200)\
  # In case you specify and additional path, the scheme is required: http://localhost:9200/path\
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200\
  hosts: ["10.3.0.4:9200"]\
  username: "elastic"\
  password: "changeme" \
\
  # Set gzip compression level.\
  #compression_level: 0\
\
  # Configure escaping HTML symbols in strings.\
  #escape_html: false\
\
  # Optional protocol and basic auth credentials.\
  #protocol: "https"\
  #username: "elastic"\
  #password: "changeme"\
\
  # Dictionary of HTTP parameters to pass within the URL with index operations.\
  #parameters:\
    #param1: value1\
    #param2: value2\
\
  # Number of workers per Elasticsearch host.\
  #worker: 1\
\
  # Optional index name. The default is "filebeat" plus date\
  # and generates [filebeat-]YYYY.MM.DD keys.\
  # In case you modify this pattern you must update setup.template.name and setup.template.pattern accordingly.\
  #index: "filebeat-%\{[agent.version]\}-%\{+yyyy.MM.dd\}"\
\
  # Optional ingest node pipeline. By default no pipeline will be used.\
  #pipeline: ""\
\
  # Optional HTTP path\
  #path: "/elasticsearch"\
\
  # Custom HTTP headers to add to each request\
  #headers:\
  #  X-My-Header: Contents of the header\
\
  # Proxy server URL\
  #proxy_url: http://proxy:3128\
\
  # Whether to disable proxy settings for outgoing connections. If true, this\
  # takes precedence over both the proxy_url field and any environment settings\
  # (HTTP_PROXY, HTTPS_PROXY). The default is false.\
  #proxy_disable: false\
\
  # The number of times a particular Elasticsearch index operation is attempted. If\
  # the indexing operation doesn't succeed after this many retries, the events are\
  # dropped. The default is 3.\
  #max_retries: 3\
\
  # The maximum number of events to bulk in a single Elasticsearch bulk API index request.\
  # The default is 50.\
  #bulk_max_size: 50\
\
  # The number of seconds to wait before trying to reconnect to Elasticsearch\
  # after a network error. After waiting backoff.init seconds, the Beat\
  # tries to reconnect. If the attempt fails, the backoff timer is increased\
  # exponentially up to backoff.max. After a successful connection, the backoff\
  # timer is reset. The default is 1s.\
  #backoff.init: 1s\
\
  # The maximum number of seconds to wait before attempting to connect to\
  # Elasticsearch after a network error. The default is 60s.\
  #backoff.max: 60s\
\
  # Configure HTTP request timeout before failing a request to Elasticsearch.\
  #timeout: 90\
\
  # Use SSL settings for HTTPS.\
  #ssl.enabled: true\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL-based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions from 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client certificate key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the certificate key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE-based cipher suites\
  #ssl.curve_types: []\
\
  # Configure what types of renegotiation are supported. Valid options are\
  # never, once, and freely. Default is never.\
  #ssl.renegotiation: never\
\
#----------------------------- Logstash output ---------------------------------\
#output.logstash:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # The Logstash hosts\
  #hosts: ["localhost:5044"]\
\
  # Number of workers per Logstash host.\
  #worker: 1\
\
  # Set gzip compression level.\
  #compression_level: 3\
\
  # Configure escaping HTML symbols in strings.\
  #escape_html: false\
\
  # Optional maximum time to live for a connection to Logstash, after which the\
  # connection will be re-established.  A value of `0s` (the default) will\
  # disable this feature.\
  #\
  # Not yet supported for async connections (i.e. with the "pipelining" option set)\
  #ttl: 30s\
\
  # Optionally load-balance events between Logstash hosts. Default is false.\
  #loadbalance: false\
\
  # Number of batches to be sent asynchronously to Logstash while processing\
  # new batches.\
  #pipelining: 2\
\
  # If enabled only a subset of events in a batch of events is transferred per\
  # transaction.  The number of events to be sent increases up to `bulk_max_size`\
  # if no error is encountered.\
  #slow_start: false\
\
  # The number of seconds to wait before trying to reconnect to Logstash\
  # after a network error. After waiting backoff.init seconds, the Beat\
  # tries to reconnect. If the attempt fails, the backoff timer is increased\
  # exponentially up to backoff.max. After a successful connection, the backoff\
  # timer is reset. The default is 1s.\
  #backoff.init: 1s\
\
  # The maximum number of seconds to wait before attempting to connect to\
  # Logstash after a network error. The default is 60s.\
  #backoff.max: 60s\
\
  # Optional index name. The default index name is set to filebeat\
  # in all lowercase.\
  #index: 'filebeat'\
\
  # SOCKS5 proxy server URL\
  #proxy_url: socks5://user:password@socks5-server:2233\
\
  # Resolve names locally when using a proxy server. Defaults to false.\
  #proxy_use_local_resolver: false\
\
  # Enable SSL support. SSL is automatically enabled if any SSL setting is set.\
  #ssl.enabled: true\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions from 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # Optional SSL configuration options. SSL is off by default.\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client certificate key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the Certificate Key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE-based cipher suites\
  #ssl.curve_types: []\
\
  # Configure what types of renegotiation are supported. Valid options are\
  # never, once, and freely. Default is never.\
  #ssl.renegotiation: never\
\
  # The number of times to retry publishing an event after a publishing failure.\
  # After the specified number of retries, the events are typically dropped.\
  # Some Beats, such as Filebeat and Winlogbeat, ignore the max_retries setting\
  # and retry until all events are published.  Set max_retries to a value less\
  # than 0 to retry until all events are published. The default is 3.\
  #max_retries: 3\
\
  # The maximum number of events to bulk in a single Logstash request. The\
  # default is 2048.\
  #bulk_max_size: 2048\
\
  # The number of seconds to wait for responses from the Logstash server before\
  # timing out. The default is 30s.\
  #timeout: 30s\
\
#------------------------------- Kafka output ----------------------------------\
#output.kafka:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # The list of Kafka broker addresses from which to fetch the cluster metadata.\
  # The cluster metadata contain the actual Kafka brokers events are published\
  # to.\
  #hosts: ["localhost:9092"]\
\
  # The Kafka topic used for produced events. The setting can be a format string\
  # using any event field. To set the topic from document type use `%\{[type]\}`.\
  #topic: beats\
\
  # The Kafka event key setting. Use format string to create a unique event key.\
  # By default no event key will be generated.\
  #key: ''\
\
  # The Kafka event partitioning strategy. Default hashing strategy is `hash`\
  # using the `output.kafka.key` setting or randomly distributes events if\
  # `output.kafka.key` is not configured.\
  #partition.hash:\
    # If enabled, events will only be published to partitions with reachable\
    # leaders. Default is false.\
    #reachable_only: false\
\
    # Configure alternative event field names used to compute the hash value.\
    # If empty `output.kafka.key` setting will be used.\
    # Default value is empty list.\
    #hash: []\
\
  # Authentication details. Password is required if username is set.\
  #username: ''\
  #password: ''\
\
  # Kafka version Filebeat is assumed to run against. Defaults to the "1.0.0".\
  #version: '1.0.0'\
\
  # Configure JSON encoding\
  #codec.json:\
    # Pretty-print JSON event\
    #pretty: false\
\
    # Configure escaping HTML symbols in strings.\
    #escape_html: false\
\
  # Metadata update configuration. Metadata contains leader information\
  # used to decide which broker to use when publishing.\
  #metadata:\
    # Max metadata request retry attempts when cluster is in middle of leader\
    # election. Defaults to 3 retries.\
    #retry.max: 3\
\
    # Wait time between retries during leader elections. Default is 250ms.\
    #retry.backoff: 250ms\
\
    # Refresh metadata interval. Defaults to every 10 minutes.\
    #refresh_frequency: 10m\
\
    # Strategy for fetching the topics metadata from the broker. Default is false.\
    #full: false\
\
  # The number of concurrent load-balanced Kafka output workers.\
  #worker: 1\
\
  # The number of times to retry publishing an event after a publishing failure.\
  # After the specified number of retries, events are typically dropped.\
  # Some Beats, such as Filebeat, ignore the max_retries setting and retry until\
  # all events are published.  Set max_retries to a value less than 0 to retry\
  # until all events are published. The default is 3.\
  #max_retries: 3\
\
  # The maximum number of events to bulk in a single Kafka request. The default\
  # is 2048.\
  #bulk_max_size: 2048\
\
  # Duration to wait before sending bulk Kafka request. 0 is no delay. The default\
  # is 0.\
  #bulk_flush_frequency: 0s\
\
  # The number of seconds to wait for responses from the Kafka brokers before\
  # timing out. The default is 30s.\
  #timeout: 30s\
\
  # The maximum duration a broker will wait for number of required ACKs. The\
  # default is 10s.\
  #broker_timeout: 10s\
\
  # The number of messages buffered for each Kafka broker. The default is 256.\
  #channel_buffer_size: 256\
\
  # The keep-alive period for an active network connection. If 0s, keep-alives\
  # are disabled. The default is 0 seconds.\
  #keep_alive: 0\
\
  # Sets the output compression codec. Must be one of none, snappy and gzip. The\
  # default is gzip.\
  #compression: gzip\
\
  # Set the compression level. Currently only gzip provides a compression level\
  # between 0 and 9. The default value is chosen by the compression algorithm.\
  #compression_level: 4\
\
  # The maximum permitted size of JSON-encoded messages. Bigger messages will be\
  # dropped. The default value is 1000000 (bytes). This value should be equal to\
  # or less than the broker's message.max.bytes.\
  #max_message_bytes: 1000000\
\
  # The ACK reliability level required from broker. 0=no response, 1=wait for\
  # local commit, -1=wait for all replicas to commit. The default is 1.  Note:\
  # If set to 0, no ACKs are returned by Kafka. Messages might be lost silently\
  # on error.\
  #required_acks: 1\
\
  # The configurable ClientID used for logging, debugging, and auditing\
  # purposes.  The default is "beats".\
  #client_id: beats\
\
  # Enable SSL support. SSL is automatically enabled if any SSL setting is set.\
  #ssl.enabled: true\
\
  # Optional SSL configuration options. SSL is off by default.\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions from 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client Certificate Key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the Certificate Key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE-based cipher suites\
  #ssl.curve_types: []\
\
  # Configure what types of renegotiation are supported. Valid options are\
  # never, once, and freely. Default is never.\
  #ssl.renegotiation: never\
\
#------------------------------- Redis output ----------------------------------\
#output.redis:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # Configure JSON encoding\
  #codec.json:\
    # Pretty print json event\
    #pretty: false\
\
    # Configure escaping HTML symbols in strings.\
    #escape_html: false\
\
  # The list of Redis servers to connect to. If load-balancing is enabled, the\
  # events are distributed to the servers in the list. If one server becomes\
  # unreachable, the events are distributed to the reachable servers only.\
  #hosts: ["localhost:6379"]\
\
  # The name of the Redis list or channel the events are published to. The\
  # default is filebeat.\
  #key: filebeat\
\
  # The password to authenticate to Redis with. The default is no authentication.\
  #password:\
\
  # The Redis database number where the events are published. The default is 0.\
  #db: 0\
\
  # The Redis data type to use for publishing events. If the data type is list,\
  # the Redis RPUSH command is used. If the data type is channel, the Redis\
  # PUBLISH command is used. The default value is list.\
  #datatype: list\
\
  # The number of workers to use for each host configured to publish events to\
  # Redis. Use this setting along with the loadbalance option. For example, if\
  # you have 2 hosts and 3 workers, in total 6 workers are started (3 for each\
  # host).\
  #worker: 1\
\
  # If set to true and multiple hosts or workers are configured, the output\
  # plugin load balances published events onto all Redis hosts. If set to false,\
  # the output plugin sends all events to only one host (determined at random)\
  # and will switch to another host if the currently selected one becomes\
  # unreachable. The default value is true.\
  #loadbalance: true\
\
  # The Redis connection timeout in seconds. The default is 5 seconds.\
  #timeout: 5s\
\
  # The number of times to retry publishing an event after a publishing failure.\
  # After the specified number of retries, the events are typically dropped.\
  # Some Beats, such as Filebeat, ignore the max_retries setting and retry until\
  # all events are published. Set max_retries to a value less than 0 to retry\
  # until all events are published. The default is 3.\
  #max_retries: 3\
\
  # The number of seconds to wait before trying to reconnect to Redis\
  # after a network error. After waiting backoff.init seconds, the Beat\
  # tries to reconnect. If the attempt fails, the backoff timer is increased\
  # exponentially up to backoff.max. After a successful connection, the backoff\
  # timer is reset. The default is 1s.\
  #backoff.init: 1s\
\
  # The maximum number of seconds to wait before attempting to connect to\
  # Redis after a network error. The default is 60s.\
  #backoff.max: 60s\
\
  # The maximum number of events to bulk in a single Redis request or pipeline.\
  # The default is 2048.\
  #bulk_max_size: 2048\
\
  # The URL of the SOCKS5 proxy to use when connecting to the Redis servers. The\
  # value must be a URL with a scheme of socks5://.\
  #proxy_url:\
\
  # This option determines whether Redis hostnames are resolved locally when\
  # using a proxy. The default value is false, which means that name resolution\
  # occurs on the proxy server.\
  #proxy_use_local_resolver: false\
\
  # Enable SSL support. SSL is automatically enabled, if any SSL setting is set.\
  #ssl.enabled: true\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # Optional SSL configuration options. SSL is off by default.\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client Certificate Key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the Certificate Key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE based cipher suites\
  #ssl.curve_types: []\
\
  # Configure what types of renegotiation are supported. Valid options are\
  # never, once, and freely. Default is never.\
  #ssl.renegotiation: never\
\
#------------------------------- File output -----------------------------------\
#output.file:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # Configure JSON encoding\
  #codec.json:\
    # Pretty-print JSON event\
    #pretty: false\
\
    # Configure escaping HTML symbols in strings.\
    #escape_html: false\
\
  # Path to the directory where to save the generated files. The option is\
  # mandatory.\
  #path: "/tmp/filebeat"\
\
  # Name of the generated files. The default is `filebeat` and it generates\
  # files: `filebeat`, `filebeat.1`, `filebeat.2`, etc.\
  #filename: filebeat\
\
  # Maximum size in kilobytes of each file. When this size is reached, and on\
  # every Filebeat restart, the files are rotated. The default value is 10240\
  # kB.\
  #rotate_every_kb: 10000\
\
  # Maximum number of files under path. When this number of files is reached,\
  # the oldest file is deleted and the rest are shifted from last to first. The\
  # default is 7 files.\
  #number_of_files: 7\
\
  # Permissions to use for file creation. The default is 0600.\
  #permissions: 0600\
\
#----------------------------- Console output ---------------------------------\
#output.console:\
  # Boolean flag to enable or disable the output module.\
  #enabled: true\
\
  # Configure JSON encoding\
  #codec.json:\
    # Pretty-print JSON event\
    #pretty: false\
\
    # Configure escaping HTML symbols in strings.\
    #escape_html: false\
\
#================================= Paths ======================================\
\
# The home path for the Filebeat installation. This is the default base path\
# for all other path settings and for miscellaneous files that come with the\
# distribution (for example, the sample dashboards).\
# If not set by a CLI flag or in the configuration file, the default for the\
# home path is the location of the binary.\
#path.home:\
\
# The configuration path for the Filebeat installation. This is the default\
# base path for configuration files, including the main YAML configuration file\
# and the Elasticsearch template file. If not set by a CLI flag or in the\
# configuration file, the default for the configuration path is the home path.\
#path.config: $\{path.home\}\
\
# The data path for the Filebeat installation. This is the default base path\
# for all the files in which Filebeat needs to store its data. If not set by a\
# CLI flag or in the configuration file, the default for the data path is a data\
# subdirectory inside the home path.\
#path.data: $\{path.home\}/data\
\
# The logs path for a Filebeat installation. This is the default location for\
# the Beat's log files. If not set by a CLI flag or in the configuration file,\
# the default for the logs path is a logs subdirectory inside the home path.\
#path.logs: $\{path.home\}/logs\
\
#================================ Keystore ==========================================\
# Location of the Keystore containing the keys and their sensitive values.\
#keystore.path: "$\{path.config\}/beats.keystore"\
\
#============================== Dashboards =====================================\
# These settings control loading the sample dashboards to the Kibana index. Loading\
# the dashboards are disabled by default and can be enabled either by setting the\
# options here, or by using the `-setup` CLI flag or the `setup` command.\
#setup.dashboards.enabled: false\
\
# The directory from where to read the dashboards. The default is the `kibana`\
# folder in the home path.\
#setup.dashboards.directory: $\{path.home\}/kibana\
\
# The URL from where to download the dashboards archive. It is used instead of\
# the directory if it has a value.\
#setup.dashboards.url:\
\
# The file archive (zip file) from where to read the dashboards. It is used instead\
# of the directory when it has a value.\
#setup.dashboards.file:\
\
# In case the archive contains the dashboards from multiple Beats, this lets you\
# select which one to load. You can load all the dashboards in the archive by\
# setting this to the empty string.\
#setup.dashboards.beat: filebeat\
\
# The name of the Kibana index to use for setting the configuration. Default is ".kibana"\
#setup.dashboards.kibana_index: .kibana\
\
# The Elasticsearch index name. This overwrites the index name defined in the\
# dashboards and index pattern. Example: testbeat-*\
#setup.dashboards.index:\
\
# Always use the Kibana API for loading the dashboards instead of autodetecting\
# how to install the dashboards by first querying Elasticsearch.\
#setup.dashboards.always_kibana: false\
\
# If true and Kibana is not reachable at the time when dashboards are loaded,\
# it will retry to reconnect to Kibana instead of exiting with an error.\
#setup.dashboards.retry.enabled: false\
\
# Duration interval between Kibana connection retries.\
#setup.dashboards.retry.interval: 1s\
\
# Maximum number of retries before exiting with an error, 0 for unlimited retrying.\
#setup.dashboards.retry.maximum: 0\
\
\
#============================== Template =====================================\
\
# A template is used to set the mapping in Elasticsearch\
# By default template loading is enabled and the template is loaded.\
# These settings can be adjusted to load your own template or overwrite existing ones.\
\
# Set to false to disable template loading.\
#setup.template.enabled: true\
\
# Template name. By default the template name is "filebeat-%\{[agent.version]\}"\
# The template name and pattern has to be set in case the Elasticsearch index pattern is modified.\
#setup.template.name: "filebeat-%\{[agent.version]\}"\
\
# Template pattern. By default the template pattern is "-%\{[agent.version]\}-*" to apply to the default index settings.\
# The first part is the version of the beat and then -* is used to match all daily indices.\
# The template name and pattern has to be set in case the Elasticsearch index pattern is modified.\
#setup.template.pattern: "filebeat-%\{[agent.version]\}-*"\
\
# Path to fields.yml file to generate the template\
#setup.template.fields: "$\{path.config\}/fields.yml"\
\
# A list of fields to be added to the template and Kibana index pattern. Also\
# specify setup.template.overwrite: true to overwrite the existing template.\
# This setting is experimental.\
#setup.template.append_fields:\
#- name: field_name\
#  type: field_type\
\
# Enable JSON template loading. If this is enabled, the fields.yml is ignored.\
#setup.template.json.enabled: false\
\
# Path to the JSON template file\
#setup.template.json.path: "$\{path.config\}/template.json"\
\
# Name under which the template is stored in Elasticsearch\
#setup.template.json.name: ""\
\
# Overwrite existing template\
#setup.template.overwrite: false\
\
# Elasticsearch template settings\
setup.template.settings:\
\
  # A dictionary of settings to place into the settings.index dictionary\
  # of the Elasticsearch template. For more details, please check\
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html\
  #index:\
    #number_of_shards: 1\
    #codec: best_compression\
    #number_of_routing_shards: 30\
\
  # A dictionary of settings for the _source field. For more details, please check\
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html\
  #_source:\
    #enabled: false\
\
#============================== Setup ILM =====================================\
\
# Configure index lifecycle management (ILM). These settings create a write\
# alias and add additional settings to the index template. When ILM is enabled,\
# output.elasticsearch.index is ignored, and the write alias is used to set the\
# index name.\
\
# Enable ILM support. Valid values are true, false, and auto. When set to auto\
# (the default), the Beat uses index lifecycle management when it connects to a\
# cluster that supports ILM; otherwise, it creates daily indices.\
#setup.ilm.enabled: auto\
\
# Set the prefix used in the index lifecycle write alias name. The default alias\
# name is 'filebeat-%\{[agent.version]\}'.\
#setup.ilm.rollover_alias: "filebeat"\
\
# Set the rollover index pattern. The default is "%\{now/d\}-000001".\
#setup.ilm.pattern: "\{now/d\}-000001"\
\
# Set the lifecycle policy name. The default policy name is\
# 'filebeat-%\{[agent.version]\}'.\
#setup.ilm.policy_name: "mypolicy"\
\
# The path to a JSON file that contains a lifecycle policy configuration. Used\
# to load your own lifecycle policy.\
#setup.ilm.policy_file:\
\
# Disable the check for an existing lifecycle policy. The default is false. If\
# you disable this check, set setup.ilm.overwrite: true so the lifecycle policy\
# can be installed.\
#setup.ilm.check_exists: false\
\
# Overwrite the lifecycle policy at startup. The default is false.\
#setup.ilm.overwrite: false\
\
#============================== Kibana =====================================\
\
# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.\
# This requires a Kibana endpoint configuration.\
setup.kibana:\
  host: "10.3.0.4:5601" # TODO: Change this to the IP address of your ELK server\
  # Kibana Host\
  # Scheme and port can be left out and will be set to the default (http and 5601)\
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path\
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601\
  #host: "localhost:5601"\
\
  # Optional protocol and basic auth credentials.\
  #protocol: "https"\
  #username: "elastic"\
  #password: "changeme"\
\
  # Optional HTTP path\
  #path: ""\
\
  # Use SSL settings for HTTPS. Default is true.\
  #ssl.enabled: true\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions from 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # SSL configuration. The default is off.\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client certificate key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the certificate key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE-based cipher suites\
  #ssl.curve_types: []\
\
\
\
#================================ Logging ======================================\
# There are four options for the log output: file, stderr, syslog, eventlog\
# The file output is the default.\
\
# Sets log level. The default log level is info.\
# Available log levels are: error, warning, info, debug\
#logging.level: info\
\
# Enable debug output for selected components. To enable all selectors use ["*"]\
# Other available selectors are "beat", "publish", "service"\
# Multiple selectors can be chained.\
#logging.selectors: [ ]\
\
# Send all logging output to stderr. The default is false.\
#logging.to_stderr: false\
\
# Send all logging output to syslog. The default is false.\
#logging.to_syslog: false\
\
# Send all logging output to Windows Event Logs. The default is false.\
#logging.to_eventlog: false\
\
# If enabled, Filebeat periodically logs its internal metrics that have changed\
# in the last period. For each metric that changed, the delta from the value at\
# the beginning of the period is logged. Also, the total values for\
# all non-zero internal metrics are logged on shutdown. The default is true.\
#logging.metrics.enabled: true\
\
# The period after which to log the internal metrics. The default is 30s.\
#logging.metrics.period: 30s\
\
# Logging to rotating files. Set logging.to_files to false to disable logging to\
# files.\
logging.to_files: true\
logging.files:\
  # Configure the path where the logs are written. The default is the logs directory\
  # under the home path (the binary location).\
  #path: /var/log/filebeat\
\
  # The name of the files where the logs are written to.\
  #name: filebeat\
\
  # Configure log file size limit. If limit is reached, log file will be\
  # automatically rotated\
  #rotateeverybytes: 10485760 # = 10MB\
\
  # Number of rotated log files to keep. Oldest files will be deleted first.\
  #keepfiles: 7\
\
  # The permissions mask to apply when rotating log files. The default value is 0600.\
  # Must be a valid Unix-style file permissions mask expressed in octal notation.\
  #permissions: 0600\
\
  # Enable log file rotation on time intervals in addition to size-based rotation.\
  # Intervals must be at least 1s. Values of 1m, 1h, 24h, 7*24h, 30*24h, and 365*24h\
  # are boundary-aligned with minutes, hours, days, weeks, months, and years as\
  # reported by the local system clock. All other intervals are calculated from the\
  # Unix epoch. Defaults to disabled.\
  #interval: 0\
\
  # Rotate existing logs on startup rather than appending to the existing\
  # file. Defaults to true.\
  # rotateonstartup: true\
\
# Set to true to log messages in JSON format.\
#logging.json: false\
\
\
#============================== X-Pack Monitoring ===============================\
# Filebeat can export internal metrics to a central Elasticsearch monitoring\
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The\
# reporting is disabled by default.\
\
# Set to true to enable the monitoring reporter.\
#monitoring.enabled: false\
\
# Sets the UUID of the Elasticsearch cluster under which monitoring data for this\
# Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch\
# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.\
#monitoring.cluster_uuid:\
\
# Uncomment to send the metrics to Elasticsearch. Most settings from the\
# Elasticsearch output are accepted here as well.\
# Note that the settings should point to your Elasticsearch *monitoring* cluster.\
# Any setting that is not set is automatically inherited from the Elasticsearch\
# output configuration, so if you have the Elasticsearch output configured such\
# that it is pointing to your Elasticsearch monitoring cluster, you can simply\
# uncomment the following line.\
#monitoring.elasticsearch:\
\
  # Array of hosts to connect to.\
  # Scheme and port can be left out and will be set to the default (http and 9200)\
  # In case you specify and additional path, the scheme is required: http://localhost:9200/path\
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200\
  #hosts: ["localhost:9200"]\
\
  # Set gzip compression level.\
  #compression_level: 0\
\
  # Optional protocol and basic auth credentials.\
  #protocol: "https"\
  #username: "beats_system"\
  #password: "changeme"\
\
  # Dictionary of HTTP parameters to pass within the URL with index operations.\
  #parameters:\
    #param1: value1\
    #param2: value2\
\
  # Custom HTTP headers to add to each request\
  #headers:\
  #  X-My-Header: Contents of the header\
\
  # Proxy server url\
  #proxy_url: http://proxy:3128\
\
  # The number of times a particular Elasticsearch index operation is attempted. If\
  # the indexing operation doesn't succeed after this many retries, the events are\
  # dropped. The default is 3.\
  #max_retries: 3\
\
  # The maximum number of events to bulk in a single Elasticsearch bulk API index request.\
  # The default is 50.\
  #bulk_max_size: 50\
\
  # The number of seconds to wait before trying to reconnect to Elasticsearch\
  # after a network error. After waiting backoff.init seconds, the Beat\
  # tries to reconnect. If the attempt fails, the backoff timer is increased\
  # exponentially up to backoff.max. After a successful connection, the backoff\
  # timer is reset. The default is 1s.\
  #backoff.init: 1s\
\
  # The maximum number of seconds to wait before attempting to connect to\
  # Elasticsearch after a network error. The default is 60s.\
  #backoff.max: 60s\
\
  # Configure HTTP request timeout before failing an request to Elasticsearch.\
  #timeout: 90\
\
  # Use SSL settings for HTTPS.\
  #ssl.enabled: true\
\
  # Configure SSL verification mode. If `none` is configured, all server hosts\
  # and certificates will be accepted. In this mode, SSL based connections are\
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is\
  # `full`.\
  #ssl.verification_mode: full\
\
  # List of supported/valid TLS versions. By default all TLS versions from 1.0 up to\
  # 1.2 are enabled.\
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]\
\
  # SSL configuration. The default is off.\
  # List of root certificates for HTTPS server verifications\
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]\
\
  # Certificate for SSL client authentication\
  #ssl.certificate: "/etc/pki/client/cert.pem"\
\
  # Client certificate key\
  #ssl.key: "/etc/pki/client/cert.key"\
\
  # Optional passphrase for decrypting the certificate key.\
  #ssl.key_passphrase: ''\
\
  # Configure cipher suites to be used for SSL connections\
  #ssl.cipher_suites: []\
\
  # Configure curve types for ECDHE-based cipher suites\
  #ssl.curve_types: []\
\
  # Configure what types of renegotiation are supported. Valid options are\
  # never, once, and freely. Default is never.\
  #ssl.renegotiation: never\
\
  #metrics.period: 10s\
  #state.period: 1m\
\
#================================ HTTP Endpoint ======================================\
# Each beat can expose internal metrics through a HTTP endpoint. For security\
# reasons the endpoint is disabled by default. This feature is currently experimental.\
# Stats can be access through http://localhost:5066/stats . For pretty JSON output\
# append ?pretty to the URL.\
\
# Defines if the HTTP endpoint is enabled.\
#http.enabled: false\
\
# The HTTP endpoint will bind to this hostname, IP address, unix socket or named pipe.\
# When using IP addresses, it is recommended to only use localhost.\
#http.host: localhost\
\
# Port on which the HTTP endpoint will bind. Default is 5066.\
#http.port: 5066\
\
# Define which user should be owning the named pipe.\
#http.named_pipe.user:\
\
# Define which the permissions that should be applied to the named pipe, use the Security\
# Descriptor Definition Language (SDDL) to define the permission. This option cannot be used with\
# `http.user`.\
#http.named_pipe.security_descriptor:\
\
#============================= Process Security ================================\
\
# Enable or disable seccomp system call filtering on Linux. Default is enabled.\
#seccomp.enabled: true\
\
#================================= Migration ==================================\
\
# This allows to enable 6.7 migration aliases\
#migration.6_to_7.enabled: false\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# cd roles\
root@d519ff78ed01:/etc/ansible/roles# ls\
filebeat-playbook.yml  
\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2 \
root@d519ff78ed01:/etc/ansible/roles# ls\
filebeat-playbook.yml  
\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2 \
root@d519ff78ed01:/etc/ansible/roles# nano filebeat-playbook.yml \
root@d519ff78ed01:/etc/ansible/roles# cd ..\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# nano ansible.cfg \
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# nano ansible.cfg \
root@d519ff78ed01:/etc/ansible# la\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# cd ..\
root@d519ff78ed01:/etc# ls \
adduser.conf            group         libaudit.conf  passwd       
\f1\b \cf5 security
\f0\b0 \cf2 \

\f1\b \cf5 alternatives
\f0\b0 \cf2             group-        login.defs     profile      
\f1\b \cf5 selinux
\f0\b0 \cf2 \

\f1\b \cf5 ansible
\f0\b0 \cf2                  gshadow       
\f1\b \cf5 logrotate.d
\f0\b0 \cf2     
\f1\b \cf5 profile.d
\f0\b0 \cf2     shadow\

\f1\b \cf5 apt
\f0\b0 \cf2                      gshadow-      lsb-release    
\f1\b \cf5 python
\f0\b0 \cf2        shells\
bash.bashrc             
\f1\b \cf5 gss
\f0\b0 \cf2            machine-id     
\f1\b \cf5 python2.7
\f0\b0 \cf2     
\f1\b \cf5 skel
\f0\b0 \cf2 \
bindresvport.blacklist  host.conf     mailcap        
\f1\b \cf5 python3
\f0\b0 \cf2       
\f1\b \cf5 ssh
\f0\b0 \cf2 \

\f1\b \cf5 ca-certificates
\f0\b0 \cf2          hostname      mailcap.order  
\f1\b \cf5 python3.6
\f0\b0 \cf2     
\f1\b \cf5 ssl
\f0\b0 \cf2 \
ca-certificates.conf    hosts         
\f1\b \cf5 metricbeat
\f0\b0 \cf2      
\f1\b \cf5 rc0.d
\f0\b0 \cf2         subgid\

\f1\b \cf5 cron.daily
\f0\b0 \cf2               
\f1\b \cf5 init.d
\f0\b0 \cf2         mime.types     
\f1\b \cf5 rc1.d
\f0\b0 \cf2         subuid\

\f1\b \cf5 dbus-1
\f0\b0 \cf2                   inputrc       mke2fs.conf    
\f1\b \cf5 rc2.d
\f0\b0 \cf2         sysctl.conf\
debconf.conf            issue         
\f1\b \cf6 mtab
\f0\b0 \cf2            
\f1\b \cf5 rc3.d
\f0\b0 \cf2         
\f1\b \cf5 sysctl.d
\f0\b0 \cf2 \
debian_version          issue.net     nanorc         
\f1\b \cf5 rc4.d
\f0\b0 \cf2         
\f1\b \cf5 systemd
\f0\b0 \cf2 \

\f1\b \cf5 default
\f0\b0 \cf2                  
\f1\b \cf5 kernel
\f0\b0 \cf2         networks       
\f1\b \cf5 rc5.d
\f0\b0 \cf2         
\f1\b \cf5 terminfo
\f0\b0 \cf2 \
deluser.conf            ld.so.cache   nsswitch.conf  
\f1\b \cf5 rc6.d
\f0\b0 \cf2         
\f1\b \cf5 update-motd.d
\f0\b0 \cf2 \

\f1\b \cf5 dpkg
\f0\b0 \cf2                     ld.so.conf    
\f1\b \cf5 opt
\f0\b0 \cf2             
\f1\b \cf5 rcS.d
\f0\b0 \cf2         
\f1\b \cf5 vim
\f0\b0 \cf2 \
environment             
\f1\b \cf5 ld.so.conf.d
\f0\b0 \cf2   
\f1\b \cf6 os-release
\f0\b0 \cf2      resolv.conf\
fstab                   
\f1\b \cf5 ldap
\f0\b0 \cf2           pam.conf       
\f1\b \cf3 rmt
\f0\b0 \cf2 \
gai.conf                legal         
\f1\b \cf5 pam.d
\f0\b0 \cf2           securetty\
root@d519ff78ed01:/etc# cd metricbeat/\
root@d519ff78ed01:/etc/metricbeat# ls\

\f1\b \cf5 elasticsearch-7.6.2
\f0\b0 \cf2                       metricbeat-playbook.yml\

\f1\b \cf4 elasticsearch-7.6.2-linux-x86_64.tar.gz
\f0\b0 \cf2   metricbeat.yml\

\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2                metricbeath-configuration.yml\
metricbeat-config.yml                    metricbeath.yml\
root@d519ff78ed01:/etc/metricbeat# cd ..\
root@d519ff78ed01:/etc# cd ansible/\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg          
\f1\b \cf5 files
\f0\b0 \cf2   install-elk.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
filebeat-config.yml  hosts  pentest.yml\
root@d519ff78ed01:/etc/ansible# nano install-elk.yml \
\
\cf7 \cb2   GNU nano 2.9.3                   install-elk.yml                              \cf2 \cb1 \
\
client_loop: send disconnect: Broken pipe\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\cf8 \cb9   [Restored 11 Mar 2021 at 13:08:06]\cf2 \
\cb1 Last login: Thu Mar 11 13:08:06 on ttys002\
Restored session: Thu 11 Mar 2021 13:01:26 AEDT\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % ssh azadmin@23.100.39.172\
^C\
gez@miMac monu-mel-cyber-pt-11-2020-u-c % ssh azadmin@40.78.60.247 \
The authenticity of host '40.78.60.247 (40.78.60.247)' can't be established.\
ECDSA key fingerprint is SHA256:rpu4NdxYIWK+ywvgdAIVSXwyK8XwiNuRbL3YcP8nP3g.\
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes\
Warning: Permanently added '40.78.60.247' (ECDSA) to the list of known hosts.\
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-azure x86_64)\
\
 * Documentation:  https://help.ubuntu.com\
 * Management:     https://landscape.canonical.com\
 * Support:        https://ubuntu.com/advantage\
\
  System information as of Fri Mar 12 03:36:23 UTC 2021\
\
  System load:  0.0                Processes:           117\
  Usage of /:   19.7% of 28.90GB   Users logged in:     0\
  Memory usage: 3%                 IP address for eth0: 10.0.0.5\
  Swap usage:   0%\
\
 * Introducing self-healing high availability clusters in MicroK8s.\
   Simple, hardened, Kubernetes for production, from RaspberryPi to DC.\
\
     https://microk8s.io/high-availability\
\
 * Canonical Livepatch is available for installation.\
   - Reduce system reboots and improve kernel security. Activate at:\
     https://ubuntu.com/livepatch\
\
15 packages can be updated.\
0 of these updates are security updates.\
To see these additional updates run: apt list --upgradable\
\
\
Last login: Wed Mar 10 09:14:05 2021 from 172.197.53.164\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ssh azadmin@40.112.197.58\
 \
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ssh azadmin@10.0.0.7\
The authenticity of host '10.0.0.7 (10.0.0.7)' can't be established.\
ECDSA key fingerprint is SHA256:O7Royq4aXkIUKcaleQdxawQAFF1FCEUcDHiS/E1HJZg.\
Are you sure you want to continue connecting (yes/no)? yes\
Warning: Permanently added '10.0.0.7' (ECDSA) to the list of known hosts.\
azadmin@10.0.0.7: Permission denied (publickey).\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ for ssh azadmin@10.0.0.7\
-bash: syntax error near unexpected token `azadmin@10.0.0.7'\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while : azadmin@10.0.0.7\
> ^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while : do echo azadmin@10.0.07 sleep 1 done\
> ^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'azadmin@10.0.07'; sleep 1; done\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'azadmin@10.0.07'; sleep 1; done\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
^Xazadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
azadmin@10.0.07\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker start xenodochial_chaplygin\
xenodochial_chaplygin\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker start xenodochial_chaplygin\
xenodochial_chaplygin\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'azadmin@10.0.07'; sleep 1; done\
azadmin@10.0.07\
azadmin@10.0.07\
^Xazadmin@10.0.07\
azadmin@10.0.07\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker attach xenodochial_chaplygin\
root@d519ff78ed01:~# sudo apt install stress\
bash: sudo: command not found\
root@d519ff78ed01:~# ssh azadmin@10.0.0.7\
Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-azure x86_64)\
\
 * Documentation:  https://help.ubuntu.com\
 * Management:     https://landscape.canonical.com\
 * Support:        https://ubuntu.com/advantage\
\
  System information as of Fri Mar 12 03:57:22 UTC 2021\
\
  System load:  0.0                Processes:              125\
  Usage of /:   12.2% of 28.90GB   Users logged in:        0\
  Memory usage: 27%                IP address for eth0:    10.0.0.7\
  Swap usage:   0%                 IP address for docker0: 172.17.0.1\
\
 * Introducing self-healing high availability clusters in MicroK8s.\
   Simple, hardened, Kubernetes for production, from RaspberryPi to DC.\
\
     https://microk8s.io/high-availability\
\
 * Canonical Livepatch is available for installation.\
   - Reduce system reboots and improve kernel security. Activate at:\
     https://ubuntu.com/livepatch\
\
15 packages can be updated.\
0 of these updates are security updates.\
To see these additional updates run: apt list --upgradable\
\
\
Last login: Mon Mar  8 11:03:38 2021 from 10.0.0.5\
azadmin@Web-1:~$ sudo apt install stress\
Reading package lists... Done\
Building dependency tree       \
Reading state information... Done\
The following NEW packages will be installed:\
  stress\
0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\
Need to get 17.5 kB of archives.\
After this operation, 46.1 kB of additional disk space will be used.\
Get:1 http://azure.archive.ubuntu.com/ubuntu bionic/universe amd64 stress amd64 1.0.4-2 [17.5 kB]\
Fetched 17.5 kB in 0s (1406 kB/s)\
Selecting previously unselected package stress.\
(Reading database ... 93209 files and directories currently installed.)\
Preparing to unpack .../stress_1.0.4-2_amd64.deb ...\
Unpacking stress (1.0.4-2) ...\
Setting up stress (1.0.4-2) ...\
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\
Processing triggers for install-info (6.5.0.dfsg.1-2) ...\
azadmin@Web-1:~$ sudo stress --cpu 1\
stress: info: [31811] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd\
^C\
azadmin@Web-1:~$ exit\
logout\
Connection to 10.0.0.7 closed.\
root@d519ff78ed01:~# ssh azadmin@10.0.0.sudo apt install stress \
ssh: Could not resolve hostname 10.0.0.sudo: Name or service not known\
root@d519ff78ed01:~# ssh azadmin@10.0.0.8                       \
''Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1040-azure x86_64)\
\
 * Documentation:  https://help.ubuntu.com\
 * Management:     https://landscape.canonical.com\
 * Support:        https://ubuntu.com/advantage\
\
  System information as of Fri Mar 12 04:03:08 UTC 2021\
\
  System load:  0.17               Processes:              125\
  Usage of /:   12.2% of 28.90GB   Users logged in:        0\
  Memory usage: 25%                IP address for eth0:    10.0.0.8\
  Swap usage:   0%                 IP address for docker0: 172.17.0.1\
\
 * Introducing self-healing high availability clusters in MicroK8s.\
   Simple, hardened, Kubernetes for production, from RaspberryPi to DC.\
\
     https://microk8s.io/high-availability\
\
 * Canonical Livepatch is available for installation.\
   - Reduce system reboots and improve kernel security. Activate at:\
     https://ubuntu.com/livepatch\
\
15 packages can be updated.\
0 of these updates are security updates.\
To see these additional updates run: apt list --upgradable\
\
\
Last login: Mon Mar  8 11:03:38 2021 from 10.0.0.5\
azadmin@web2:~$ 'sudo apt install stress \
> ^C\
azadmin@web2:~$ sudo apt install stress \
Reading package lists... Done\
Building dependency tree       \
Reading state information... Done\
The following NEW packages will be installed:\
  stress\
0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\
Need to get 17.5 kB of archives.\
After this operation, 46.1 kB of additional disk space will be used.\
Get:1 http://azure.archive.ubuntu.com/ubuntu bionic/universe amd64 stress amd64 1.0.4-2 [17.5 kB]\
Fetched 17.5 kB in 0s (402 kB/s)\
Selecting previously unselected package stress.\
(Reading database ... 93209 files and directories currently installed.)\
Preparing to unpack .../stress_1.0.4-2_amd64.deb ...\
Unpacking stress (1.0.4-2) ...\
Setting up stress (1.0.4-2) ...\
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\
Processing triggers for install-info (6.5.0.dfsg.1-2) ...\
azadmin@web2:~$ sudo stress --cpu 1\
stress: info: [31851] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd\
^C\
azadmin@web2:~$ exit\
logout\
Connection to 10.0.0.8 closed.\
root@d519ff78ed01:~# wget 10.0.0.7     \
bash: wget: command not found\
root@d519ff78ed01:~# exit\
exit\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ wget 10.0.0.7\
--2021-03-12 04:08:46--  http://10.0.0.7/\
Connecting to 10.0.0.7:80... connected.\
HTTP request sent, awaiting response... 302 Found\
Location: login.php [following]\
--2021-03-12 04:08:46--  http://10.0.0.7/login.php\
Reusing existing connection to 10.0.0.7:80.\
HTTP request sent, awaiting response... 200 OK\
Length: 1415 (1.4K) [text/html]\
Saving to: \'91index.html\'92\
\
index.html          100%[===================>]   1.38K  --.-KB/s    in 0s      \
\
2021-03-12 04:08:46 (131 MB/s) - \'91index.html\'92 saved [1415/1415]\
\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'wget10.0.0.7'; sleep 1; done\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
wget10.0.0.7\
^Xwget10.0.0.7\
wget10.0.0.7\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'wget 10.0.0.7'; sleep 1; done\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
wget 10.0.0.7\
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ wget 10.0.0.7\
--2021-03-12 04:21:51--  http://10.0.0.7/\
Connecting to 10.0.0.7:80... connected.\
HTTP request sent, awaiting response... 302 Found\
Location: login.php [following]\
--2021-03-12 04:21:51--  http://10.0.0.7/login.php\
Reusing existing connection to 10.0.0.7:80.\
HTTP request sent, awaiting response... 200 OK\
Length: 1415 (1.4K) [text/html]\
Saving to: \'91index.html.1\'92\
\
index.html.1          100%[======================>]   1.38K  --.-KB/s    in 0s      \
\
2021-03-12 04:21:51 (198 MB/s) - \'91index.html.1\'92 saved [1415/1415]\
\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html  index.html.1\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'wget 10.0.0.7 <enter> '; sleep 1; done\
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
ewget 10.0.0.7 <enter> \
wget 10.0.0.7 <enter> \
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ls\
index.html  index.html.1\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ while :; do echo 'wget 10.0.0.7 press <enter> '; sleep 1; done\
wget 10.0.0.7 press <enter> \
wget 10.0.0.7 press <enter> \
wget 10.0.0.7 press <enter> \
^Xwget 10.0.0.7 press <enter> \
wget 10.0.0.7 press <enter> \
^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ wget 10.0.0.7\
--2021-03-12 04:25:40--  http://10.0.0.7/\
Connecting to 10.0.0.7:80... connected.\
HTTP request sent, awaiting response... 302 Found\
Location: login.php [following]\
--2021-03-12 04:25:40--  http://10.0.0.7/login.php\
Reusing existing connection to 10.0.0.7:80.\
HTTP request sent, awaiting response... 200 OK\
Length: 1415 (1.4K) [text/html]\
Saving to: \'91index.html.2\'92\
\
index.html.2          100%[======================>]   1.38K  --.-KB/s    in 0s      \
\
2021-03-12 04:25:40 (237 MB/s) - \'91index.html.2\'92 saved [1415/1415]\
\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ ^C\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ for wget 10.0.0.7; do wget 10.0.0.7\
-bash: syntax error near unexpected token `10.0.0.7'\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ for wget 10.0.0.7; do wget 10.0.0.7; done\
-bash: syntax error near unexpected token `10.0.0.7'\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ for wget 10.0.0.7\
-bash: syntax error near unexpected token `10.0.0.7'\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ cd etc\
-bash: cd: etc: No such file or directory\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ cd /etc/ansible\
-bash: cd: /etc/ansible: No such file or directory\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker start xenodochial_chaplygin\
xenodochial_chaplygin\

\f1\b \cf3 azadmin@jump-box-provisioner
\f0\b0 \cf2 :
\f1\b \cf5 ~
\f0\b0 \cf2 $ sudo docker attach xenodochial_chaplygin\
root@d519ff78ed01:~# cd /etc\
root@d519ff78ed01:/etc# cd\
root@d519ff78ed01:~# cd et/c\
bash: cd: et/c: No such file or directory\
root@d519ff78ed01:~# cd /etc\
root@d519ff78ed01:/etc# ls\
adduser.conf            group         libaudit.conf  passwd       
\f1\b \cf5 security
\f0\b0 \cf2 \

\f1\b \cf5 alternatives
\f0\b0 \cf2             group-        login.defs     profile      
\f1\b \cf5 selinux
\f0\b0 \cf2 \

\f1\b \cf5 ansible
\f0\b0 \cf2                  gshadow       
\f1\b \cf5 logrotate.d
\f0\b0 \cf2     
\f1\b \cf5 profile.d
\f0\b0 \cf2     shadow\

\f1\b \cf5 apt
\f0\b0 \cf2                      gshadow-      lsb-release    
\f1\b \cf5 python
\f0\b0 \cf2        shells\
bash.bashrc             
\f1\b \cf5 gss
\f0\b0 \cf2            machine-id     
\f1\b \cf5 python2.7
\f0\b0 \cf2     
\f1\b \cf5 skel
\f0\b0 \cf2 \
bindresvport.blacklist  host.conf     mailcap        
\f1\b \cf5 python3
\f0\b0 \cf2       
\f1\b \cf5 ssh
\f0\b0 \cf2 \

\f1\b \cf5 ca-certificates
\f0\b0 \cf2          hostname      mailcap.order  
\f1\b \cf5 python3.6
\f0\b0 \cf2     
\f1\b \cf5 ssl
\f0\b0 \cf2 \
ca-certificates.conf    hosts         
\f1\b \cf5 metricbeat
\f0\b0 \cf2      
\f1\b \cf5 rc0.d
\f0\b0 \cf2         subgid\

\f1\b \cf5 cron.daily
\f0\b0 \cf2               
\f1\b \cf5 init.d
\f0\b0 \cf2         mime.types     
\f1\b \cf5 rc1.d
\f0\b0 \cf2         subuid\

\f1\b \cf5 dbus-1
\f0\b0 \cf2                   inputrc       mke2fs.conf    
\f1\b \cf5 rc2.d
\f0\b0 \cf2         sysctl.conf\
debconf.conf            issue         
\f1\b \cf6 mtab
\f0\b0 \cf2            
\f1\b \cf5 rc3.d
\f0\b0 \cf2         
\f1\b \cf5 sysctl.d
\f0\b0 \cf2 \
debian_version          issue.net     nanorc         
\f1\b \cf5 rc4.d
\f0\b0 \cf2         
\f1\b \cf5 systemd
\f0\b0 \cf2 \

\f1\b \cf5 default
\f0\b0 \cf2                  
\f1\b \cf5 kernel
\f0\b0 \cf2         networks       
\f1\b \cf5 rc5.d
\f0\b0 \cf2         
\f1\b \cf5 terminfo
\f0\b0 \cf2 \
deluser.conf            ld.so.cache   nsswitch.conf  
\f1\b \cf5 rc6.d
\f0\b0 \cf2         
\f1\b \cf5 update-motd.d
\f0\b0 \cf2 \

\f1\b \cf5 dpkg
\f0\b0 \cf2                     ld.so.conf    
\f1\b \cf5 opt
\f0\b0 \cf2             
\f1\b \cf5 rcS.d
\f0\b0 \cf2         
\f1\b \cf5 vim
\f0\b0 \cf2 \
environment             
\f1\b \cf5 ld.so.conf.d
\f0\b0 \cf2   
\f1\b \cf6 os-release
\f0\b0 \cf2      resolv.conf\
fstab                   
\f1\b \cf5 ldap
\f0\b0 \cf2           pam.conf       
\f1\b \cf3 rmt
\f0\b0 \cf2 \
gai.conf                legal         
\f1\b \cf5 pam.d
\f0\b0 \cf2           securetty\
root@d519ff78ed01:/etc# cd metricbeat/\
root@d519ff78ed01:/etc/metricbeat# ls\

\f1\b \cf5 elasticsearch-7.6.2
\f0\b0 \cf2                       metricbeat-playbook.yml\

\f1\b \cf4 elasticsearch-7.6.2-linux-x86_64.tar.gz
\f0\b0 \cf2   metricbeat.yml\

\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2                metricbeath-configuration.yml\
metricbeat-config.yml                    metricbeath.yml\
root@d519ff78ed01:/etc/metricbeat# nano metricbeat-config.yml \
root@d519ff78ed01:/etc/metricbeat# nano metricbeat-config.yml \
root@d519ff78ed01:/etc/metricbeat# nano metricbeat-config.yml \
root@d519ff78ed01:/etc/metricbeat# ls\

\f1\b \cf5 elasticsearch-7.6.2
\f0\b0 \cf2                       metricbeat-playbook.yml\

\f1\b \cf4 elasticsearch-7.6.2-linux-x86_64.tar.gz
\f0\b0 \cf2   metricbeat.yml\

\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2                metricbeath-configuration.yml\
metricbeat-config.yml                    metricbeath.yml\
root@d519ff78ed01:/etc/metricbeat# nano metricbeat.yml\
root@d519ff78ed01:/etc/metricbeat# ls\

\f1\b \cf5 elasticsearch-7.6.2
\f0\b0 \cf2                       metricbeat-playbook.yml\

\f1\b \cf4 elasticsearch-7.6.2-linux-x86_64.tar.gz
\f0\b0 \cf2   metricbeat.yml\

\f1\b \cf4 metricbeat-7.6.1-amd64.deb
\f0\b0 \cf2                metricbeath-configuration.yml\
metricbeat-config.yml                    metricbeath.yml\
root@d519ff78ed01:/etc/metricbeat# nano metricbeat-playbook.yml \
root@d519ff78ed01:/etc/metricbeat# cd /ansible\
bash: cd: /ansible: No such file or directory\
root@d519ff78ed01:/etc/metricbeat# cd ..\
root@d519ff78ed01:/etc# cd ansible\
root@d519ff78ed01:/etc/ansible# ls\
ansible.cfg  filebeat-config.yml  
\f1\b \cf5 files
\f0\b0 \cf2   hosts  install-elk.yml  pentest.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
root@d519ff78ed01:/etc/ansible# nano filebeat-config.yml \
root@d519ff78ed01:/etc/ansible# nano install-elk.yml \
root@d519ff78ed01:/etc/ansible# ls   \
ansible.cfg  filebeat-config.yml  
\f1\b \cf5 files
\f0\b0 \cf2   hosts  install-elk.yml  pentest.yml  
\f1\b \cf5 roles
\f0\b0 \cf2 \
root@d519ff78ed01:/etc/ansible# nano install-elk.yml \
\
\cf7 \cb2   GNU nano 2.9.3                       install-elk.yml                        Modified  \cf2 \cb1 \
\
\
---\
- name: Configure Elk VM with Docker\
  hosts: elk\
  remote_user: azadmin\
  become: True\
  tasks:\
\cf6     # Use apt module\cf2 \
    - name: Install docker.io\
      apt:\
        update_cache: yes\
        force_apt_get: yes\
        name: docker.io\
        state: present\
\
\cf6       # Use apt module\cf2 \
    - name: Install python3-pip\
      apt:\
        force_apt_get: yes\
        name: python3-pip\
        state: present\
\
\cf6       # Use pip module (It will default to pip3)\cf2 \
    - name: Install Docker module\
      pip:\
        name: docker\
        state: present\
\
\cf6       # Use command module\cf2 \
    - name: Increase virtual memory\
      command: sysctl -w vm.max_map_count=262144\
\
\cf6       # Use sysctl module\cf2 \
    - name: Use more memory\
      sysctl:\
        name: vm.max_map_count\
        value: '262144'\
        state: present\
        reload: yes\
\
\cf6       # Use docker_container module\cf2 \
    - name: download and launch a docker elk container\
      docker_container:\
        name: elk\
        image: sebp/elk:761\
        state: started\
        restart_policy: always\
\cf6         # Please list the ports that ELK runs on\cf2 \
        published_ports:\
          -  5601:5601\
          -  9200:9200\
          -  5044:5044\
                                   \cf7 \cb2 [ Read 62 lines ]\cf2 \cb1 \
\cf7 \cb2 ^G\cf2 \cb1  Get Help   \cf7 \cb2 ^O\cf2 \cb1  Write Out  \cf7 \cb2 ^W\cf2 \cb1  Where Is   \cf7 \cb2 ^K\cf2 \cb1  Cut Text   \cf7 \cb2 ^J\cf2 \cb1  Justify    \cf7 \cb2 ^C\cf2 \cb1  Cur Pos\
\cf7 \cb2 ^X\cf2 \cb1  Exit       \cf7 \cb2 ^R\cf2 \cb1  Read File  \cf7 \cb2 ^\\\cf2 \cb1  Replace    \cf7 \cb2 ^U\cf2 \cb1  Uncut Text \cf7 \cb2 ^T\cf2 \cb1  To Spell   \cf7 \cb2 ^_\cf2 \cb1  Go To Line\
}